<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>BASEMENT PAPERS</title>
        <style>
</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        
        
        
    </head>
    <body class="vscode-light">
        <ul>
<li><a href="#basement-papers">BASEMENT PAPERS</a>
<ul>
<li><a href="#deepcut-joint-subset-partition-and-labeling-for-multi-person-pose-estimation">DeepCut: Joint subset partition and labeling for multi person pose estimation</a></li>
<li><a href="#deepercut-a-deeper-stronger-and-faster-multi-person-pose-estimation-model">DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</a></li>
</ul>
</li>
<li><a href="#papers-in-2017">Papers in 2017</a>
<ul>
<li><a href="#densereg-fully-convolutional-dense-shape-regression-in-the-wild">DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild</a></li>
<li><a href="#densepose-dense-human-pose-estimation-in-the-wild">DensePose: Dense Human Pose Estimation In The Wild</a></li>
<li><a href="#posetrack-joint-multi-person-pose-estimation-and-tracking">PoseTrack: Joint multi-person pose estimation and tracking</a></li>
<li><a href="#rmpe-regional-multi-person-pose-estimation">RMPE: Regional Multi-person Pose Estimation</a></li>
<li><a href="#arttrack-articulated-multi-person-tracking-in-the-wild">ArtTrack: Articulated multi-person tracking in the wild</a></li>
</ul>
</li>
<li><a href="#papers-in-2018">Papers in 2018</a>
<ul>
<li><a href="#weakly-and-semi-supervised-human-body-part-parsing-via-pose-guided-knowledge-transfer">Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer</a></li>
<li><a href="#2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning">2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</a></li>
<li><a href="#human-pose-estimation-with-parsing-induced-learner-xuecheng">Human Pose Estimation with Parsing Induced Learner Xuecheng</a></li>
<li><a href="#optical-non-line-of-sight-physics-based-3d-human-pose-estimation">Optical Non-Line-of-Sight Physics-based 3D Human Pose Estimation</a></li>
<li><a href="#pose-partition-networks-for-multi-person-pose-estimation">Pose Partition Networks for Multi-Person Pose Estimation</a></li>
<li><a href="#dense-pose-transfer">Dense Pose Transfer</a></li>
<li><a href="#pose-proposal-networks">Pose Proposal Networks</a></li>
<li><a href="#learning-to-estimate-3d-human-pose-and-shape-from-a-single-color-image">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</a></li>
<li><a href="#fast-human-pose-estimation">Fast Human Pose Estimation</a></li>
<li><a href="#posehd-boosting-human-detectors-using-human-pose-information">PosehD: Boosting human detectors using human pose information</a></li>
<li><a href="#multiposenet-fast-multi-person-pose-estimation-using-pose-residual-network">MultiPoseNet: Fast Multi-Person Pose Estimation Using Pose Residual Network</a></li>
<li><a href="#personlab-person-pose-estimation-and-instance-segmentation-with-a-bottom-up-part-based-geometric-embedding-model">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</a></li>
<li><a href="#unsupervised-geometry-aware-representation-for-3d-human-pose-estimation">Unsupervised geometry-aware representation for 3D human pose estimation</a></li>
<li><a href="#propagating-lstm-3d-pose-estimation-based-on-joint-interdependency">Propagating LSTM: 3D pose estimation based on joint interdependency</a></li>
<li><a href="#deeply-learned-compositional-models-for-human-pose-estimation">Deeply Learned Compositional Models for Human Pose Estimation</a></li>
<li><a href="#learning-3d-human-pose-from-structure-and-motion">Learning 3D human pose from structure and motion</a></li>
<li><a href="#mutual-learning-to-adapt-for-joint-human-parsing-and-pose-estimation">Mutual Learning to Adapt for Joint Human Parsing and Pose Estimation</a></li>
<li><a href="#exploiting-temporal-information-for-3d-human-pose-estimation">Exploiting temporal information for 3D human pose estimation</a></li>
<li><a href="#inner-space-preserving-generative-pose-machine">Inner Space Preserving Generative Pose Machine</a></li>
<li><a href="#a-minimal-closed-form-solution-for-multi-perspective-pose-estimation-using-points-and-lines">A Minimal Closed-Form Solution for Multi-perspective Pose Estimation using Points and Lines</a></li>
<li><a href="#integral-human-pose-regression">Integral Human Pose Regression</a></li>
<li><a href="#accelerating-dynamic-programs-via-nested-benders-decomposition-with-application-to-multi-person-pose-estimation">Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation</a></li>
<li><a href="#deep-autoencoder-for-combined-human-pose-estimation-and-body-model-upscaling">Deep autoencoder for combined human pose estimation and body model upscaling</a></li>
<li><a href="#object-occluded-human-shape-and-pose-estimation-from-a-single-color-image">Object-Occluded Human Shape and Pose Estimation from a Single Color Image</a></li>
<li><a href="#multistage-adversarial-losses-for-pose-based-human-image-synthesis">Multistage Adversarial Losses for Pose-Based Human Image Synthesis</a></li>
<li><a href="#detect-and-track-efficient-pose-estimation-in-videos">Detect-and-Track: Efficient Pose Estimation in Videos</a></li>
<li><a href="#3d-pose-estimation-and-3d-model-retrieval-for-objects-in-the-wild">3D Pose Estimation and 3D Model Retrieval for Objects in the Wild</a></li>
<li><a href="#3d-human-pose-estimation-in-the-wild-by-adversarial-learning">3D Human Pose Estimation in the Wild by Adversarial Learning</a></li>
<li><a href="#end-to-end-recovery-of-human-shape-and-pose">End-to-End Recovery of Human Shape and Pose</a></li>
<li><a href="#learning-monocular-3d-human-pose-estimation-from-multi-view-images">Learning Monocular 3D Human Pose Estimation from Multi-view Images</a></li>
<li><a href="#shape-and-pose-estimation-for-closely-interacting-persons-using-multi-view-images">Shape and Pose Estimation for Closely Interacting Persons Using Multi-view Images</a></li>
<li><a href="#personlab-person-pose-estimation-and-instance-segmentation-with-a-bottom-up-part-based-geometric-embedding-model-1">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</a></li>
<li><a href="#simple-baselines-for-human-pose-estimation-and-tracking">Simple baselines for human pose estimation and tracking</a></li>
<li><a href="#cascaded-pyramid-network-for-multi-person-pose-estimation">Cascaded Pyramid Network for Multi-person Pose Estimation</a></li>
<li><a href="#deep-network-for-the-integrated-3d-sensing-of-multiple-people-in-natural-images">Deep network for the integrated 3D sensing of multiple people in natural images</a></li>
<li><a href="#monocular-3d-pose-and-shape-estimation-of-multiple-people-in-natural-scenes-the-importance-of-multiple-scene-constraints">Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes: The Importance of Multiple Scene Constraints</a></li>
</ul>
</li>
<li><a href="#papers-in-2019">Papers in 2019</a>
<ul>
<li><a href="#domes-to-drones-self-supervised-active-triangulation-for-3d-human-pose-reconstruction">Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction</a></li>
<li><a href="#learning-temporal-pose-estimation-from-sparsely-labeled-videos">Learning Temporal Pose Estimation from Sparsely-Labeled Videos</a></li>
<li><a href="#cross-view-fusion-for-3d-human-pose-estimation">Cross view fusion for 3D human pose estimation</a></li>
<li><a href="#dynamic-kernel-distillation-for-efficient-pose-estimation-in-videos">Dynamic kernel distillation for efficient pose estimation in videos</a></li>
<li><a href="#denserac-joint-3d-pose-and-shape-estimation-by-dense-render-and-compare">DenseRaC: Joint 3D pose and shape estimation by dense render-and-compare</a></li>
<li><a href="#shape-aware-human-pose-and-shape-reconstruction-using-multi-view-images">Shape-aware human pose and shape reconstruction using multi-view images</a></li>
<li><a href="#monocular-3d-human-pose-estimation-by-generation-and-ordinal-ranking">Monocular 3D human pose estimation by generation and ordinal ranking</a></li>
<li><a href="#single-stage-multi-person-pose-machines">Single-stage multi-person pose machines</a></li>
<li><a href="#learning-to-reconstruct-3d-human-pose-and-shape-via-model-fitting-in-the-loop">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</a></li>
<li><a href="#higherhrnet-scale-aware-representation-learning-for-bottom-up-human-pose-estimation">HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</a></li>
<li><a href="#distribution-aware-coordinate-representation-for-human-pose-estimation">Distribution-Aware Coordinate Representation for Human Pose Estimation</a></li>
<li><a href="#multi-path-learning-for-object-pose-estimation-across-domains">Multi-path Learning for Object Pose Estimation Across Domains</a></li>
<li><a href="#graph-embedded-pose-clustering-for-anomaly-detection">Graph Embedded Pose Clustering for Anomaly Detection</a></li>
<li><a href="#just-go-with-the-flow-self-supervised-scene-flow-estimation">Just Go with the Flow: Self-Supervised Scene Flow Estimation</a></li>
<li><a href="#extreme-relative-pose-network-under-hybrid-representations">Extreme Relative Pose Network under Hybrid Representations</a></li>
<li><a href="#the-devil-is-in-the-details-delving-into-unbiased-data-processing-for-human-pose-estimation">The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation</a></li>
<li><a href="#factorized-higher-order-cnns-with-an-application-to-spatio-temporal-emotion-estimation">Factorized Higher-Order CNNs with an Application to Spatio-Temporal Emotion Estimation</a></li>
<li><a href="#multiview-consistent-semi-supervised-learning-for-3d-human-pose-estimation">Multiview-Consistent Semi-Supervised Learning for 3D Human Pose Estimation</a></li>
<li><a href="#self-supervised-learning-of-3d-human-pose-using-multi-view-geometry">Self-supervised learning of 3D human pose using multi-view geometry</a></li>
<li><a href="#vibe-video-inference-for-human-body-pose-and-shape-estimation">VIBE: Video Inference for Human Body Pose and Shape Estimation</a></li>
<li><a href="#multi-person-pose-estimation-with-enhanced-channel-wise-and-spatial-information">Multi-person pose estimation with enhanced channel-wise and spatial information</a></li>
<li><a href="#deep-high-resolution-representation-learning-for-human-pose-estimation">Deep high-resolution representation learning for human pose estimation</a></li>
<li><a href="#does-learning-specific-features-for-related-parts-help-human-pose-estimation">Does learning specific features for related parts help human pose estimation?</a></li>
<li><a href="#3d-human-pose-estimation-in-video-with-temporal-convolutions-and-semi-supervised-training">3D human pose estimation in video with temporal convolutions and semi-supervised training</a></li>
<li><a href="#semantic-graph-convolutional-networks-for-3d-human-pose-regression">Semantic graph convolutional networks for 3D human pose regression</a></li>
<li><a href="#ige-net-inverse-graphics-energy-networks-for-human-pose-estimation-and-single-view-reconstruction">IGE-net: Inverse graphics energy networks for human pose estimation and single-view reconstruction</a></li>
<li><a href="#pa3d-pose-action-3d-machine-for-video-recognition">PA3D: Pose-action 3D machine for video recognition</a></li>
<li><a href="#generating-multiple-hypotheses-for-3d-human-pose-estimation-with-mixture-density-network">Generating multiple hypotheses for 3D human pose estimation with mixture density network</a></li>
<li><a href="#unsupervised-3d-pose-estimation-with-geometric-self-supervision">Unsupervised 3D pose estimation with geometric self-supervision</a></li>
<li><a href="#posefix-model-agnostic-general-human-pose-refinement-network">Posefix: Model-agnostic general human pose refinement network</a></li>
<li><a href="#deepfashion2-a-versatile-benchmark-for-detection-pose-estimation-segmentation-and-re-identification-of-clothing-images">Deepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images</a></li>
<li><a href="#pifpaf-composite-fields-for-human-pose-estimation">PifPaf: Composite fields for human pose estimation</a></li>
<li><a href="#exploiting-spatial-temporal-relationships-for-3d-pose-estimation-via-graph-convolutional-networks">Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks</a></li>
<li><a href="#distill-knowledge-from-nrsfm-for-weakly-supervised-3d-pose-learning">Distill knowledge from NRSfM for weakly supervised 3D pose learning</a></li>
<li><a href="#occlusion-aware-networks-for-3d-human-pose-estimation-in-video">Occlusion-aware networks for 3D human pose estimation in video</a></li>
<li><a href="#moulding-humans-non-parametric-3d-human-shape-estimation-from-single-images">Moulding humans: Non-parametric 3D human shape estimation from single images</a></li>
<li><a href="#optimizing-network-structure-for-3d-human-pose-estimation">Optimizing network structure for 3D human pose estimation</a></li>
<li><a href="#gp2c-geometric-projection-parameter-consensus-for-joint-3d-pose-and-focal-length-estimation-in-the-wild">GP2C: Geometric projection parameter consensus for joint 3D pose and focal length estimation in the wild</a></li>
<li><a href="#a2j-anchor-to-joint-regression-network-for-3d-articulated-pose-estimation-from-a-single-depth-image">A2J: Anchor-to-joint regression network for 3D articulated pose estimation from a single depth image</a></li>
<li><a href="#xr-egopose-egocentric-3d-human-pose-from-an-hmd-camera">XR-EgoPose: Egocentric 3D human pose from an HMD camera</a></li>
<li><a href="#panoptic-studio-a-massively-multiview-system-for-social-interaction-capture">Panoptic Studio: A Massively Multiview System for Social Interaction Capture</a></li>
<li><a href="#end-to-end-learning-for-graph-decomposition">End-to-end learning for graph decomposition</a></li>
<li><a href="#fast-and-robust-multi-person-3d-pose-estimation-from-multiple-views">Fast and robust multi-person 3D pose estimation from multiple views</a></li>
</ul>
</li>
<li><a href="#5-papers-in-2020">5. Papers in 2020</a>
<ul>
<li><a href="#lightweight-multi-view-3d-pose-estimation-through-camera-disentangled-representation">Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation</a></li>
<li><a href="#hope-net-a-graph-based-model-for-hand-object-pose-estimation">HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation</a></li>
<li><a href="#neural-pose-transfer-by-spatially-adaptive-instance-normalization">Neural Pose Transfer by Spatially Adaptive Instance Normalization</a></li>
<li><a href="#unipose-unified-human-pose-estimation-in-single-images-and-videos">UniPose: Unified Human Pose Estimation in Single Images and Videos</a></li>
<li><a href="#metafuse-a-pre-trained-fusion-model-for-human-pose-estimation">MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation</a></li>
<li><a href="#d3vo-deep-depth-deep-pose-and-deep-uncertainty-for-monocular-visual-odometry">D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry</a></li>
<li><a href="#cross-view-tracking-for-multi-human-3d-pose-estimation-at-over-100-fps">Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS</a></li>
<li><a href="#where-am-i-looking-at-joint-location-and-orientation-estimation-by-cross-view-matching">Where am I looking at? Joint Location and Orientation Estimation by Cross-View Matching</a></li>
<li><a href="#bodies-at-rest-3d-human-pose-and-shape-estimation-from-a-pressure-image-using-synthetic-data">Bodies at Rest: 3D Human Pose and Shape Estimation from a Pressure Image using Synthetic Data</a></li>
<li><a href="#accurate-estimation-of-body-height-from-a-single-depth-image-via-a-four-stage-developing-network">Accurate Estimation of Body Height from a Single Depth Image via a Four-Stage Developing Network</a></li>
<li><a href="#deepfaceflow-in-the-wild-dense-3d-facial-motion-estimation">DeepFaceFlow: In-the-wild Dense 3D Facial Motion Estimation</a></li>
<li><a href="#towards-better-generalization-joint-depth-pose-learning-without-posenet">Towards Better Generalization: Joint Depth-Pose Learning without PoseNet</a></li>
<li><a href="#weakly-supervised-3d-human-pose-learning-via-multi-view-images-in-the-wild">Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild</a></li>
<li><a href="#deep-kinematics-analysis-for-monocular-3d-human-pose-estimation">Deep Kinematics Analysis for Monocular 3D Human Pose Estimation</a></li>
<li><a href="#compressed-volumetric-heatmaps-for-multi-person-3d-pose-estimation">Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation</a></li>
<li><a href="#correlating-edge-pose-with-parsing">Correlating Edge, Pose with Parsing</a></li>
<li><a href="#ghum--ghuml-generative-3d-human-shape-and-articulated-pose-models">GHUM &amp; GHUML: Generative 3D Human Shape and Articulated Pose Models</a></li>
<li><a href="#cascaded-deep-monocular-3d-human-pose-estimation-with-evolutionary-training-data">Cascaded deep monocular 3D human pose estimation with evolutionary training data</a></li>
<li><a href="#self-supervised-3d-human-pose-estimation-via-part-guided-novel-image-synthesis">Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis</a></li>
<li><a href="#combining-detection-and-tracking-for-human-pose-estimation-in-videos">Combining detection and tracking for human pose estimation in videos</a></li>
<li><a href="#hemlets-pose-learning-part-centric-heatmap-triplets-for-accurate-3d-human-pose-estimation">HEMlets pose: Learning part-centric heatmap triplets for accurate 3D human pose estimation</a></li>
<li><a href="#polarimetric-relative-pose-estimation-zhaopeng">Polarimetric Relative Pose Estimation Zhaopeng</a></li>
<li><a href="#camera-distance-aware-top-down-approach-for-3d-multi-person-pose-estimation-from-a-single-rgb-image">Camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image</a></li>
<li><a href="#resolving-3d-human-pose-ambiguities-with-3d-scene-constraints">Resolving 3D human pose ambiguities with 3D scene constraints</a></li>
<li><a href="#on-boosting-single-frame-3d-human-pose-estimation-via-monocular-videos">On boosting single-frame 3D human pose estimation via monocular videos</a></li>
<li><a href="#chirality-nets-for-human-pose-regression">Chirality Nets for Human Pose Regression</a></li>
<li><a href="#sim2real-transfer-learning-for-3d-human-pose-estimation-motion-to-the-rescue">Sim2real transfer learning for 3D human pose estimation: motion to the rescue</a></li>
<li><a href="#cross-view-person-identification-by-matching-human-poses-estimated-with-confidence-on-each-body-joint">Cross-view person identification by matching human poses estimated with confidence on each body joint</a></li>
<li><a href="#a-cascaded-inception-of-inception-network-with-attention-modulated-feature-fusion-for-human-pose-estimation">A cascaded inception of inception network with attention modulated feature fusion for human pose estimation</a></li>
<li><a href="#ego-pose-estimation-and-forecasting-as-real-time-pd-control">Ego-pose estimation and forecasting as real-time PD control</a></li>
<li><a href="#end-to-end-learning-for-graph-decomposition-1">End-to-end learning for graph decomposition</a></li>
<li><a href="#simple-baselines-for-human-pose-estimation-and-tracking-1">Simple baselines for human pose estimation and tracking</a></li>
<li><a href="#monocular-3d-pose-and-shape-estimation-of-multiple-people-in-natural-scenes-the-importance-of-multiple-scene-constraints-1">Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes: The Importance of Multiple Scene Constraints</a></li>
<li><a href="#efficient-online-multi-person-2d-pose-tracking-with-recurrent-spatio-temporal-affinity-fields">Efficient online multi-person 2D pose tracking with recurrent spatio-temporal affinity fields</a></li>
<li><a href="#multi-scale-structure-aware-network-for-human-pose-estimation">Multi-Scale Structure-Aware Network for Human Pose Estimation</a></li>
<li><a href="#cascaded-pyramid-network-for-multi-person-pose-estimation-1">Cascaded Pyramid Network for Multi-person Pose Estimation</a></li>
<li><a href="#training-cnns-with-normalized-kernels">Training CNNs with normalized kernels</a></li>
<li><a href="#towards-multi-pose-guided-virtual-try-on-network">Towards multi-pose guided virtual try-on network</a></li>
<li><a href="#posetrack-a-benchmark-for-human-pose-estimation-and-tracking">PoseTrack: A Benchmark for Human Pose Estimation and Tracking</a></li>
<li><a href="#pandanet--anchor-based-single-shot-multi-person-3d-pose-estimation">PandaNet : Anchor-Based Single-Shot Multi-Person 3D Pose Estimation</a></li>
</ul>
</li>
</ul>
<h1 id="basement-papers">BASEMENT PAPERS</h1>
<h2 id="deepcut-joint-subset-partition-and-labeling-for-multi-person-pose-estimation">DeepCut: Joint subset partition and labeling for multi person pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract">abstract</h3>
<p>This paper considers the task ofarticulated human pose estimation ofmultiple people in real world images. We pro- pose an approach that jointly solves the tasks ofdetection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity ofeach other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subse- quently estimating their body pose. We propose a partition- ing and labeling formulation ofa set ofbody-part hypotheses generated with CNN-based part detectors. Our formulation, an instance ofan integer linear program, implicitly performs non-maximum suppression on the set ofpart candidates and groups them to form configurations of body parts respect- ing geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation</p>
<hr>
<h2 id="deepercut-a-deeper-stronger-and-faster-multi-person-pose-estimation-model">DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-1">abstract</h3>
<p>The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more ef- ficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation</p>
<hr>
<h1 id="papers-in-2017">Papers in 2017</h1>
<h2 id="densereg-fully-convolutional-dense-shape-regression-in-the-wild">DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-2">abstract</h3>
<p>In this paper we propose to learn a mapping from image pixels into a dense template grid through a fully convolutional network. We formulate this task as a regression problem and train our network by leveraging upon manually annotated facial landmarks &quot;in-the-wild&quot;. We use such landmarks to establish a dense correspondence field between a three-dimensional object template and the input image, which then serves as the ground-truth for training our regression system. We show that we can combine ideas from semantic segmentation with regression networks, yielding a highly-accurate quantized regression architecture. Our system, called DenseReg, allows us to estimate dense image-to-template correspondences in a fully convolutional manner. As such our network can provide useful correspondence information as a stand-alone system, while when used as an initialization for Statistical Deformable Models we obtain landmark localization results that largely outperform the current state-of-the-art on the challenging 300W benchmark. We thoroughly evaluate our method on a host of facial analysis tasks, and demonstrate its use for other correspondence estimation tasks, such as the human body and the human ear. DenseReg code is made available at <a href="http://alpguler.com/DenseReg.html">http://alpguler.com/DenseReg.html</a> along with supplementary materials.</p>
<hr>
<h2 id="densepose-dense-human-pose-estimation-in-the-wild">DensePose: Dense Human Pose Estimation In The Wild</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-3">abstract</h3>
<p>In this work we establish dense correspondences be-
tween an RGB image and a surface-based representation ofthe human body, a task we refer to as dense human pose estimation. We gather dense correspondences for 50K per- sons appearing in the COCO dataset by introducing an ef- ficient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence ‘in the wild’, namely in the presence ofbackground, occlusions and scale variations. We improve our training set’s effec- tiveness by training an inpainting network that can fill in missing ground truth values and report improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority ofthe latter. We further improve accuracy through cascading, obtaining a system that delivers highly-accurate results at multiple frames per second on a single gpu. Supplementary mate- rials, data, code, and videos are provided on the project page <a href="http://densepose.org">http://densepose.org</a>.</p>
<hr>
<h2 id="posetrack-joint-multi-person-pose-estimation-and-tracking">PoseTrack: Joint multi-person pose estimation and tracking</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-4">abstract</h3>
<p>In this work, we introduce the challenging problem of joint multi-person pose estimation and tracking of an unknown number of persons in unconstrained videos. Existing methods for multi-person pose estimation in images cannot be applied directly to this problem, since it also requires to solve the problem of person association over time in addition to the pose estimation for each person. We therefore propose a novel method that jointly models multi-person pose estimation and tracking in a single formulation. To this end, we represent body joint detections in a video by a spatio-temporal graph and solve an integer linear program to partition the graph into sub-graphs that correspond to plausible body pose trajectories for each person. The proposed approach implicitly handles occlusion and truncation of persons. Since the problem has not been addressed quantitatively in the literature, we introduce a challenging &quot;Multi-Person PoseTrack&quot; dataset, and also propose a completely unconstrained evaluation protocol that does not make any assumptions about the scale, size, location or the number of persons. Finally, we evaluate the proposed approach and several baseline methods on our new dataset.</p>
<hr>
<h2 id="rmpe-regional-multi-person-pose-estimation">RMPE: Regional Multi-person Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-5">abstract</h3>
<p>Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76:7 mAP on the MPII (multi person) dataset[3]. Our model and source codes are made publicly available.</p>
<hr>
<h2 id="arttrack-articulated-multi-person-tracking-in-the-wild">ArtTrack: Articulated multi-person tracking in the wild</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-6">abstract</h3>
<p>In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward con-volutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public &quot;MPII Human Pose&quot; benchmark and on a new &quot;MPII Video Pose&quot; dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes1.</p>
<hr>
<h1 id="papers-in-2018">Papers in 2018</h1>
<h2 id="weakly-and-semi-supervised-human-body-part-parsing-via-pose-guided-knowledge-transfer">Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-7">abstract</h3>
<p>Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset, and we achieve state-of-the-art human parsing results. Our approach is general and can be readily extended to other object/animal parsing task assuming that their anatomical similarity can be annotated</p>
<hr>
<h2 id="2d3d-pose-estimation-and-action-recognition-using-multitask-deep-learning">2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-8">abstract</h3>
<p>Action recognition and human pose estimation are
closely related but both problems are generally handled as distinct tasks in the literature. In this work, we pro- pose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recogni- tion from video sequences. We show that a single archi- tecture can be used to solve the two problems in an effi- cient way and still achieves state-of-the-art results. Ad- ditionally, we demonstrate that optimization from end-to- end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seam- lessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effec- tiveness ofour method on the targeted tasks.</p>
<hr>
<h2 id="human-pose-estimation-with-parsing-induced-learner-xuecheng">Human Pose Estimation with Parsing Induced Learner Xuecheng</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-9">abstract</h3>
<p>Human pose estimation still faces various difficulties in
challenging scenarios. Human parsing, as a closely related task, can provide valuable cues for better pose estimation, which however has not been fully exploited. In this paper, we propose a novel Parsing Induced Learner to exploit parsing information to effectively assist pose estimation by learning to fast adapt the base pose estimation model. The proposed Parsing Induced Learner is composed ofa parsing encoder and a pose model parameter adapter, which together learn to predict dynamic parameters ofthe pose model to extract complementary useful features for more accurate pose es- timation. Comprehensive experiments on benchmarks LIP and extended PASCAL-Person-Part show that the proposed Parsing Induced Learner can improve performance ofboth single- and multi-person pose estimation to new state-of-the- art. Cross-dataset experiments also show that the proposed Parsing Induced Learner from LIP dataset can accelerate learning ofa human pose estimation model on MPII bench- mark in addition to achieving outperforming performance.</p>
<hr>
<h2 id="optical-non-line-of-sight-physics-based-3d-human-pose-estimation">Optical Non-Line-of-Sight Physics-based 3D Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-10">abstract</h3>
<p>We describe a method for 3D human pose estimation
from transient images (i.e., a 3D spatio-temporal histogram ofphotons) acquired by an optical non-line-of-sight (NLOS) imaging system. Our method can perceive 3D human pose by ‘looking around corners’ through the use of light in- directly reflected by the environment. We bring together a diverse set of technologies from NLOS imaging, human pose estimation and deep reinforcement learning to con- struct an end-to-end data processing pipeline that converts a raw stream ofphoton measurements into a full 3D human pose sequence estimate. Our contributions are the design of data representation process which includes (1) a learnable inverse point spread function (PSF) to convert raw transient images into a deep feature vector; (2) a neural humanoid control policy conditioned on the transient image feature and learned from interactions with a physics simulator; and (3) a data synthesis and augmentation strategy based on depth data that can be transferred to a real-world NLOS imaging system. Our preliminary experiments suggest that our method is able to generalize to real-world NLOS mea- surement to estimate physically-valid 3D human poses.</p>
<hr>
<h2 id="pose-partition-networks-for-multi-person-pose-estimation">Pose Partition Networks for Multi-Person Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-11">abstract</h3>
<p>This paper proposes a novel Pose Partition Network (PPN) to address the challenging multi-person pose estimation problem. The proposed PPN is favorably featured by low complexity and high ac- curacy of joint detection and partition. In particular, PPN performs dense regressions from global joint candidates within a specific embed- ding space, which is parameterized by centroids of persons, to efficiently generate robust person detection and joint partition. Then, PPN infers body joint configurations through conducting graph partition for each person detection locally, utilizing reliable global affinity cues. In this way, PPN reduces computation complexity and improves multi-person pose estimation significantly. We implement PPN with the Hourglass architecture as the backbone network to simultaneously learn joint de- tector and dense regressor. Extensive experiments on benchmarks MPII Human Pose Multi-Person, extended PASCAL-Person-Part, and WAF show the efficiency of PPN with new state-of-the-art performance.</p>
<hr>
<h2 id="dense-pose-transfer">Dense Pose Transfer</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-12">abstract</h3>
<p>In this work we integrate ideas from surface-based modeling with neural synthesis: we propose a combination of surface-based pose estimation and deep generative models that allows us to perform accurate pose transfer, i.e. synthesize a new image of a person based on a single image of that person and the image of a pose donor. We use a dense pose estimation system that maps pixels from both images to a common surface-based coordinate system, allowing the two images to be brought in correspondence with each other. We inpaint and refine the source image intensities in the surface coordinate system, prior to warping them onto the target pose. These predictions are fused with those of a convolutional predictive module through a neural synthesis module allowing for training the whole pipeline jointly end-to-end, optimizing a combination of adversarial and perceptual losses. We show that dense pose estimation is a substantially more powerful conditioning input than landmark-, or mask-based alternatives, and report systematic improvements over state of the art generators on DeepFashion and MVC datasets.</p>
<hr>
<h2 id="pose-proposal-networks">Pose Proposal Networks</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-13">abstract</h3>
<p>We propose a novel method to detect an unknown number of articu- lated 2D poses in real time. To decouple the runtime complexity of pixel-wise body part detectors from their convolutional neural network (CNN) feature map resolutions, our approach, called pose proposal networks, introduces a state-of- the-art single-shot object detection paradigm using grid-wise image feature maps in a bottom-up pose detection scenario. Body part proposals, which are repre- sented as region proposals, and limbs are detected directly via a single-shot CNN. Specialized to such detections, a bottom-up greedy parsing step is probabilisti- cally redesigned to take into account the global context. Experimental results on the MPII Multi-Person benchmark confirm that our method achieves 72.8% mAP comparable to state-of-the-art bottom-up approaches while its total runtime using a GeForce GTX1080Ti card reaches up to 5.6 ms (180 FPS), which exceeds the bottleneck runtimes that are observed in state-of-the-art approaches</p>
<hr>
<h2 id="learning-to-estimate-3d-human-pose-and-shape-from-a-single-color-image">Learning to Estimate 3D Human Pose and Shape from a Single Color Image</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-14">abstract</h3>
<p>This work addresses the problem of estimating the full
body 3D human pose and shape from a single color im- age. This is a task where iterative optimization-based so- lutions have typically prevailed, while Convolutional Net- works (ConvNets) have suffered because ofthe lack oftrain- ing data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efficient and effec- tive direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network pre- diction. Interestingly, we demonstrate that these parame- ters can be predicted reliably only from 2D keypoints and masks. These are typical outputs ofgeneric 2D human anal- ysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is em- ployed to project the 3D mesh to the image, which enables further refinement ofthe network, by optimizing for the con- sistency ofthe projection with 2D annotations (i.e., 2D key- points or masks). The proposed approach outperforms pre- vious baselines on this task and offers an attractive solution for direct prediction of3D shape from a single color image.</p>
<hr>
<h2 id="fast-human-pose-estimation">Fast Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-15">abstract</h3>
<p>Existing human pose estimation approaches often only
consider how to improve the model generalisation perfor- mance, but putting aside the significant efficiency problem. This leads to the development of heavy models with poor scalability and cost-effectiveness in practical use. In this work, we investigate the under-studied but practically crit- ical pose model efficiency problem. To this end, we present a new Fast Pose Distillation (FPD) model learning strat- egy. Specifically, the FPD trains a lightweight pose neural network architecture capable ofexecuting rapidly with low computational cost. It is achieved by effectively transferring the pose structure knowledge of a strong teacher network. Extensive evaluations demonstrate the advantages of our FPD method over a broad range ofstate-of-the-art pose es- timation approaches in terms ofmodel cost-effectiveness on two standard benchmark datasets, MPII Human Pose and Leeds Sports Pose.</p>
<hr>
<h2 id="posehd-boosting-human-detectors-using-human-pose-information">PosehD: Boosting human detectors using human pose information</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-16">abstract</h3>
<p>As most recently proposed methods for human detection have achieved a sufficiently high recall rate within a reasonable number of proposals, in this paper, we mainly focus on how to improve the precision rate of human detectors. In order to address the two main challenges in precision improvement, i.e., i) hard background instances and ii) redundant partial proposals, we propose the novel PoseHD framework, a top-down pose-based approach on the basis of an arbitrary state-of-the-art human detector. In our proposed PoseHD framework, we first make use of human pose estimation (in a batch manner) and present pose heatmap classification (by a convolutional neural network) to eliminate hard negatives by extracting the more detailed structural information; then, we utilize pose-based proposal clustering and reranking modules, filtering redundant partial proposals by comprehensively considering both holistic and part information. The experimental results on multiple pedestrian benchmark datasets validate that our proposed PoseHD framework can generally improve the overall performance of recent state-of-the-art human detectors (by 2-4% in both mAP and MR metrics). Moreover, our PoseHD framework can be easily extended to object detection with large-scale object part annotations. Finally, in this paper, we present extensive ablative analysis to compare our approach with these traditional bottom-up pose-based models and highlight the importance of our framework design decisions.</p>
<hr>
<h2 id="multiposenet-fast-multi-person-pose-estimation-using-pose-residual-network">MultiPoseNet: Fast Multi-Person Pose Estimation Using Pose Residual Network</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-17">abstract</h3>
<p>In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with ∼ 23 frames/sec.</p>
<hr>
<h2 id="personlab-person-pose-estimation-and-instance-segmentation-with-a-bottom-up-part-based-geometric-embedding-model">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-18">abstract</h3>
<p>We present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model. The proposed PersonLab model tackles both semantic-level reasoning and object-part associations using part-based modeling. Our model employs a convolutional network which learns to detect individual keypoints and predict their relative displacements, allowing us to group keypoints into person pose instances. Further, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. Our system is based on a fully-convolutional architecture and allows for efficient inference, with runtime essentially independent of the number of people present in the scene. Trained on COCO data alone, our system achieves COCO test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, significantly outperforming all previous bottom-up pose estimation systems. We are also the first bottom-up method to report competitive results for the person class in the COCO instance segmentation task, achieving a person category average precision of 0.417.</p>
<hr>
<h2 id="unsupervised-geometry-aware-representation-for-3d-human-pose-estimation">Unsupervised geometry-aware representation for 3D human pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-19">abstract</h3>
<p>Modern 3D human pose estimation techniques rely on deep networks, which require large amounts of training data. While weakly-supervised methods require less supervision, by utilizing 2D poses or multi-view imagery without annotations, they still need a sufficiently large set of samples with 3D annotations for learning to succeed. In this paper, we propose to overcome this problem by learning a geometry-aware body representation from multi-view images without annotations. To this end, we use an encoder-decoder that predicts an image from one viewpoint given an image from another viewpoint. Because this representation encodes 3D geometry, using it in a semi-supervised setting makes it easier to learn a mapping from it to 3D human pose. As evidenced by our experiments, our approach significantly outperforms fully-supervised methods given the same amount of labeled data, and improves over other semi-supervised methods while using as little as 1% of the labeled data.</p>
<hr>
<h2 id="propagating-lstm-3d-pose-estimation-based-on-joint-interdependency">Propagating LSTM: 3D pose estimation based on joint interdependency</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-20">abstract</h3>
<p>We present a novel 3D pose estimation method based on joint interdependency (JI) for acquiring 3D joints from the human pose of an RGB image. The JI incorporates the body part based structural connectivity of joints to learn the high spatial correlation of human posture on our method. Towards this goal, we propose a new long short-term memory (LSTM)-based deep learning architecture named propagating LSTM networks (p-LSTMs), where each LSTM is connected sequentially to reconstruct 3D depth from the centroid to edge joints through learning the intrinsic JI. In the first LSTM, the seed joints of 3D pose are created and reconstructed into the whole-body joints through the connected LSTMs. Utilizing the p-LSTMs, we achieve the higher accuracy of about 11.2% than state-of-the-art methods on the largest publicly available database. Importantly, we demonstrate that the JI drastically reduces the structural errors at body edges, thereby leads to a significant improvement.</p>
<hr>
<h2 id="deeply-learned-compositional-models-for-human-pose-estimation">Deeply Learned Compositional Models for Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-21">abstract</h3>
<p>Compositional models represent patterns with hierarchies of meaningful parts and subparts. Their ability to characterize high-order relationships among body parts helps resolve low-level ambiguities in human pose estimation (HPE). However, prior compositional models make unrealistic assumptions on subpart-part relationships, making them incapable to characterize complex compositional patterns. Moreover, state spaces of their higher-level parts can be exponentially large, complicating both inference and learning. To address these issues, this paper introduces a novel framework, termed as Deeply Learned Compositional Model (DLCM), for HPE. It exploits deep neural networks to learn the compositionality of human bodies. This results in a novel network with a hierarchical compositional architecture and bottom-up/top-down inference stages. In addition, we propose a novel bone-based part representation. It not only compactly encodes orientations, scales and shapes of parts, but also avoids their potentially large state spaces. With significantly lower complexities, our approach outperforms state-of-the-art methods on three benchmark datasets.</p>
<hr>
<h2 id="learning-3d-human-pose-from-structure-and-motion">Learning 3D human pose from structure and motion</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-22">abstract</h3>
<p>3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose two anatomically inspired loss functions and use them with a weakly-supervised learning framework to jointly learn from large-scale in-the-wild 2D and indoor/synthetic 3D data. We also present a simple temporal network that exploits temporal and structural cues present in predicted pose sequences to temporally harmonize the pose estimations. We carefully analyze the proposed contributions through loss surface visualizations and sensitivity analysis to facilitate deeper understanding of their working mechanism. Jointly, the two networks capture the anatomical constraints in static and kinetic states of the human body. Our complete pipeline improves the state-of-the-art by 11.8% and 12% on Human3.6M and MPI-INF-3DHP, respectively, and runs at 30 FPS on a commodity graphics card.</p>
<hr>
<h2 id="mutual-learning-to-adapt-for-joint-human-parsing-and-pose-estimation">Mutual Learning to Adapt for Joint Human Parsing and Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-23">abstract</h3>
<p>This paper presents a novel Mutual Learning to Adapt model (MuLA) for joint human parsing and pose estimation. It effectively exploits mutual benefits from both tasks and simultaneously boosts their performance. Different from existing post-processing or multi-task learning based methods, MuLA predicts dynamic task-specific model parameters via recurrently leveraging guidance information from its parallel tasks. Thus MuLA can fast adapt parsing and pose models to provide more powerful representations by incorporating information from their counterparts, giving more robust and accurate results. MuLA is implemented with convolutional neural networks and end-to-end trainable. Comprehensive experiments on benchmarks LIP and extended PASCAL-Person-Part demonstrate the effectiveness of the proposed MuLA model with superior performance to well established baselines.</p>
<hr>
<h2 id="exploiting-temporal-information-for-3d-human-pose-estimation">Exploiting temporal information for 3D human pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-24">abstract</h3>
<p>In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately 12.2 % and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.</p>
<hr>
<h2 id="inner-space-preserving-generative-pose-machine">Inner Space Preserving Generative Pose Machine</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-25">abstract</h3>
<p>Image-based generative methods, such as generative adversarial networks (GANs) have already been able to generate realistic images with much context control, specially when they are conditioned. However, most successful frameworks share a common procedure which performs an image-to-image translation with pose of figures in the image untouched. When the objective is reposing a figure in an image while preserving the rest of the image, the state-of-the-art mainly assumes a single rigid body with simple background and limited pose shift, which can hardly be extended to the images under normal settings. In this paper, we introduce an image “inner space” preserving model that assigns an interpretable low-dimensional pose descriptor (LDPD) to an articulated figure in the image. Figure reposing is then generated by passing the LDPD and the original image through multi-stage augmented hourglass networks in a conditional GAN structure, called inner space preserving generative pose machine (ISP-GPM). We evaluated ISP-GPM on reposing human figures, which are highly articulated with versatile variations. Test of a state-of-the-art pose estimator on our reposed dataset gave an accuracy over 80% on PCK0.5 metric. The results also elucidated that our ISP-GPM is able to preserve the background with high accuracy while reasonably recovering the area blocked by the figure to be reposed.</p>
<hr>
<h2 id="a-minimal-closed-form-solution-for-multi-perspective-pose-estimation-using-points-and-lines">A Minimal Closed-Form Solution for Multi-perspective Pose Estimation using Points and Lines</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-26">abstract</h3>
<p>We propose a minimal solution for pose estimation using both points and lines for a multi-perspective camera. In this paper, we treat the multi-perspective camera as a collection of rigidly attached perspective cameras. These type of imaging devices are useful for several computer vision applications that require a large coverage such as surveillance, self-driving cars, and motion-capture studios. While prior methods have considered the cases using solely points or lines, the hybrid case involving both points and lines has not been solved for multi-perspective cameras. We present the solutions for two cases. In the first case, we are given 2D to 3D correspondences for two points and one line. In the later case, we are given 2D to 3D correspondences for one point and two lines. We show that the solution for the case of two points and one line can be formulated as a fourth degree equation. This is interesting because we can get a closed-form solution and thereby achieve high computational efficiency. The later case involving two lines and one point can be mapped to an eighth degree equation. We show simulations and real experiments to demonstrate the advantages and benefits over existing methods.</p>
<hr>
<h2 id="integral-human-pose-regression">Integral Human Pose Regression</h2>
<p>Xiao
<a href="https">pdf</a></p>
<h3 id="abstract-27">abstract</h3>
<p>State-of-the-art human pose estimation methods are based on heat map representation. In spite of the good performance, the rep- resentation has a few issues in nature, such as non-differentiable post- processing and quantization error. This work shows that a simple integral operation relates and unifies the heat map representation and joint re- gression, thus avoiding the above issues. It is differentiable, efficient, and compatible with any heat map based methods. Its effectiveness is con- vincingly validated via comprehensive ablation experiments under vari- ous settings, specifically on 3D pose estimation, for the first time.</p>
<hr>
<h2 id="accelerating-dynamic-programs-via-nested-benders-decomposition-with-application-to-multi-person-pose-estimation">Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-28">abstract</h3>
<p>We present a novel approach to solve dynamic programs (DP), which are frequent in computer vision, on tree-structured graphs with exponential node state space. Typical DP approaches have to enumerate the joint state space of two adjacent nodes on every edge of the tree to compute the optimal messages. Here we propose an algorithm based on Nested Benders Decomposition (NBD) that iteratively lower-bounds the message on every edge and promises to be far more efficient. We apply our NBD algorithm along with a novel Minimum Weight Set Packing (MWSP) formulation to a multi-person pose estimation problem. While our algorithm is provably optimal at termination it operates in linear time for practical DP problems, gaining upÂ to 500 × speed up over traditional DP algorithm which have polynomial complexity.</p>
<hr>
<h2 id="deep-autoencoder-for-combined-human-pose-estimation-and-body-model-upscaling">Deep autoencoder for combined human pose estimation and body model upscaling</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-29">abstract</h3>
<p>We present a method for simultaneously estimating 3D human pose and body shape from a sparse set of wide-baseline camera views. We train a symmetric convolutional autoencoder with a dual loss that enforces learning of a latent representation that encodes skeletal joint positions, and at the same time learns a deep representation of volumetric body shape. We harness the latter to up-scale input volumetric data by a factor of 4 ×, whilst recovering a 3D estimate of joint positions with equal or greater accuracy than the state of the art. Inference runs in real-time (25 fps) and has the potential for passive human behaviour monitoring where there is a requirement for high fidelity estimation of human body shape and pose.</p>
<hr>
<h2 id="object-occluded-human-shape-and-pose-estimation-from-a-single-color-image">Object-Occluded Human Shape and Pose Estimation from a Single Color Image</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-30">abstract</h3>
<p>Occlusions between human and objects, especially for the activities of human-object interactions, are very common in practical applications. However, most of the existing approaches for 3D human shape and pose estimation require human bodies are well captured without occlusions or with minor self-occlusions. In this paper, we focus on the problem of directly estimating the object-occluded human shape and pose from single color images. Our key idea is to utilize a partial UV map to represent an object-occluded human body, and the full 3D human shape estimation is ultimately converted as an image inpainting problem. We propose a novel two-branch network architecture to train an end-to-end regressor via the latent feature supervision , which also includes a novel saliency map sub-net to extract the human information from object-occluded color images. To supervise the network training, we further build a novel dataset named as 3DOH50K. Several experiments are conducted to reveal the effectiveness of the proposed method. Experimental results demonstrate that the proposed method achieves the state-of-the-art comparing with previous methods. The dataset, codes are publicly available at <a href="https://www.yangangwang.com">https://www.yangangwang.com</a>.</p>
<hr>
<h2 id="multistage-adversarial-losses-for-pose-based-human-image-synthesis">Multistage Adversarial Losses for Pose-Based Human Image Synthesis</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-31">abstract</h3>
<p>Human image synthesis has extensive practical applications e.g. person re-identification and data augmentation for human pose estimation. However, it is much more challenging than rigid object synthesis, e.g. cars and chairs, due to the variability of human posture. In this paper, we propose a pose-based human image synthesis method which can keep the human posture unchanged in novel viewpoints. Furthermore, we adopt multistage adversarial losses separately for the foreground and background generation, which fully exploits the multi-modal characteristics of generative loss to generate more realistic looking images. We perform extensive experiments on the Human3.6M dataset and verify the effectiveness of each stage of our method. The generated human images not only keep the same pose as the input image, but also have clear detailed foreground and background. The quantitative comparison results illustrate that our approach achieves much better results than several state-of-the-art methods.</p>
<hr>
<h2 id="detect-and-track-efficient-pose-estimation-in-videos">Detect-and-Track: Efficient Pose Estimation in Videos</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-32">abstract</h3>
<p>This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection [17] and video understanding [5]. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2% on the validation and 51.8% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge [1].</p>
<hr>
<h2 id="3d-pose-estimation-and-3d-model-retrieval-for-objects-in-the-wild">3D Pose Estimation and 3D Model Retrieval for Objects in the Wild</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-33">abstract</h3>
<p>We propose a scalable, efficient and accurate approach to retrieve 3D models for objects in the wild. Our contribution is twofold. We first present a 3D pose estimation approach for object categories which significantly outperforms the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior to retrieve 3D models which accurately represent the geometry of objects in RGB images. For this purpose, we render depth images from 3D models under our predicted pose and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. In this way, we are the first to report quantitative results for 3D model retrieval on Pascal3D+, where our method chooses the same models as human annotators for 50% of the validation images on average. In addition, we show that our method, which was trained purely on Pascal3D+, retrieves rich and accurate 3D models from ShapeNet given RGB images of objects in the wild.</p>
<hr>
<h2 id="3d-human-pose-estimation-in-the-wild-by-adversarial-learning">3D Human Pose Estimation in the Wild by Adversarial Learning</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-34">abstract</h3>
<p>Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difficult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of defining hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground-truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efficacy of our adversarial learning framework with the new geometric descriptor has been demonstrated through extensive experiments on widely used public benchmarks. Our approach significantly improves the performance compared with previous state-of-the-art approaches.</p>
<hr>
<h2 id="end-to-end-recovery-of-human-shape-and-pose">End-to-End Recovery of Human Shape and Pose</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-35">abstract</h3>
<p>We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.</p>
<hr>
<h2 id="learning-monocular-3d-human-pose-estimation-from-multi-view-images">Learning Monocular 3D Human Pose Estimation from Multi-view Images</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-36">abstract</h3>
<p>Accurate 3D human pose estimation from single images is possible with sophisticated deep-net architectures that have been trained on very large datasets. However, this still leaves open the problem of capturing motions for which no such database exists. Manual annotation is tedious, slow, and error-prone. In this paper, we propose to replace most of the annotations by the use of multiple views, at training time only. Specifically, we train the system to predict the same pose in all views. Such a consistency constraint is necessary but not sufficient to predict accurate poses. We therefore complement it with a supervised loss aiming to predict the correct pose in a small set of labeled images, and with a regularization term that penalizes drift from initial predictions. Furthermore, we propose a method to estimate camera pose jointly with human pose, which lets us utilize multiview footage where calibration is difficult, e.g., for pan-tilt or moving handheld cameras. We demonstrate the effectiveness of our approach on established benchmarks, as well as on a new Ski dataset with rotating cameras and expert ski motion, for which annotations are truly hard to obtain.</p>
<hr>
<h2 id="shape-and-pose-estimation-for-closely-interacting-persons-using-multi-view-images">Shape and Pose Estimation for Closely Interacting Persons Using Multi-view Images</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-37">abstract</h3>
<p>Multi-person pose and shape estimation is very challenging, especially when the persons have close interactions. Existing methods only work well when people are well spaced out in the captured images. However, close interaction among people is a common in real life, which is more challenge due to complex articulation, frequent occlusion and inherent ambiguities. We present a fully-automatic markerless motion capture method to simultaneously estimate 3D poses and shapes of closely interacting people from multi-view sequences. We first predict the 2D joints for each person in an image, and then design a spatio-temporal tracker for multi-person pose tracking based on multi-view videos. Finally, we estimate 3D poses and shapes of all the persons with multi-view constraints using a skinned multi-person linear model (SMPL). Experimental results demonstrate that our method achieves fast but accurate pose and shape estimation results for multi-person close interaction cases. Compared with existing methods, our method does not need pre-segmentation for each person and manual intervention, which greatly reduces the complexity of the system including time complexity and system processing complexity.</p>
<hr>
<h2 id="personlab-person-pose-estimation-and-instance-segmentation-with-a-bottom-up-part-based-geometric-embedding-model-1">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-38">abstract</h3>
<p>We present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model. The proposed PersonLab model tackles both semantic-level reasoning and object-part associations using part-based modeling. Our model employs a convolutional network which learns to detect individual keypoints and predict their relative displacements, allowing us to group keypoints into person pose instances. Further, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. Our system is based on a fully-convolutional architecture and allows for efficient inference, with runtime essentially independent of the number of people present in the scene. Trained on COCO data alone, our system achieves COCO test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, significantly outperforming all previous bottom-up pose estimation systems. We are also the first bottom-up method to report competitive results for the person class in the COCO instance segmentation task, achieving a person category average precision of 0.417.</p>
<hr>
<h2 id="simple-baselines-for-human-pose-estimation-and-tracking">Simple baselines for human pose estimation and tracking</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-39">abstract</h3>
<p>There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at <a href="https://github.com/leoxiaobin/pose.pytorch">https://github.com/leoxiaobin/pose.pytorch</a>.</p>
<hr>
<h2 id="cascaded-pyramid-network-for-multi-person-pose-estimation">Cascaded Pyramid Network for Multi-person Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-40">abstract</h3>
<p>The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these 'hard' keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the 'simple' keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the 'hard' keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code1 and the detection results for person used will be publicly available for further research.</p>
<hr>
<h2 id="deep-network-for-the-integrated-3d-sensing-of-multiple-people-in-natural-images">Deep network for the integrated 3D sensing of multiple people in natural images</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-41">abstract</h3>
<p>We present MubyNet - a feed-forward, multitask, bottom up system for the integrated localization, as well as 3d pose and shape estimation, of multiple people in monocular images. The challenge is the formal modeling of the problem that intrinsically requires discrete and continuous computation, e.g. grouping people vs. predicting 3d pose. The model identifies human body structures (joints and limbs) in images, groups them based on 2d and 3d information fused using learned scoring functions, and optimally aggregates such responses into partial or complete 3d human skeleton hypotheses under kinematic tree constraints, but without knowing in advance the number of people in the scene and their visibility relations. We design a multi-task deep neural network with differentiable stages where the person grouping problem is formulated as an integer program based on learned body part scores parameterized by both 2d and 3d information. This avoids suboptimality resulting from separate 2d and 3d reasoning, with grouping performed based on the combined representation. The final stage of 3d pose and shape prediction is based on a learned attention process where information from different human body parts is optimally integrated. State-of-the-art results are obtained in large scale datasets like Human3.6M and Panoptic, and qualitatively by reconstructing the 3d shape and pose of multiple people, under occlusion, in difficult monocular images.</p>
<hr>
<h2 id="monocular-3d-pose-and-shape-estimation-of-multiple-people-in-natural-scenes-the-importance-of-multiple-scene-constraints">Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes: The Importance of Multiple Scene Constraints</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-42">abstract</h3>
<p>Human sensing has greatly benefited from recent advances in deep learning, parametric human modeling, and large scale 2d and 3d datasets. However, existing 3d models make strong assumptions about the scene, considering either a single person per image, full views of the person, a simple background or many cameras. In this paper, we leverage state-of-the-art deep multi-task neural networks and parametric human and scene modeling, towards a fully automatic monocular visual sensing system for multiple interacting people, which (i) infers the 2d and 3d pose and shape of multiple people from a single image, relying on detailed semantic representations at both model and image level, to guide a combined optimization with feedforward and feedback components, (ii) automatically integrates scene constraints including ground plane support and simultaneous volume occupancy by multiple people, and (iii) extends the single image model to video by optimally solving the temporal person assignment problem and imposing coherent temporal pose and motion reconstructions while preserving image alignment fidelity. We perform experiments on both single and multi-person datasets, and systematically evaluate each component of the model, showing improved performance and extensive multiple human sensing capability. We also apply our method to images with multiple people, severe occlusions and diverse backgrounds captured in challenging natural scenes, and obtain results of good perceptual quality.</p>
<hr>
<h1 id="papers-in-2019">Papers in 2019</h1>
<h2 id="domes-to-drones-self-supervised-active-triangulation-for-3d-human-pose-reconstruction">Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-43">abstract</h3>
<p>Existing state-of-the-art estimation systems can detect 2d poses of multiple people in images quite reliably. In contrast, 3d pose estimation from a single image is ill-posed due to occlusion and depth ambiguities. Assuming access to multiple cameras, or given an active system able to position itself to observe the scene from multiple viewpoints, reconstructing 3d pose from 2d measurements becomes well-posed within the framework of standard multi-view geometry. Less clear is what is an informative set of viewpoints for accurate 3d reconstruction, particularly in complex scenes, where people are occluded by others or by scene objects. In order to address the view selection problem in a principled way, we here introduce ACTOR, an active triangulation agent for 3d human pose reconstruction. Our fully trainable agent consists of a 2d pose estimation network (any of which would work) and a deep reinforcement learning-based policy for camera viewpoint selection. The policy predicts observation viewpoints, the number of which varies adaptively depending on scene content, and the associated images are fed to an underlying pose estimator. Importantly, training the policy requires no annotations-given a 2d pose estimator, ACTOR is trained in a self-supervised manner. In extensive evaluations on complex multi-people scenes filmed in a Panoptic dome, under multiple viewpoints, we compare our active triangulation agent to strong multi-view baselines, and show that ACTOR produces significantly more accurate 3d pose reconstructions. We also provide a proof-of-concept experiment indicating the potential of connecting our view selection policy to a physical drone observer.</p>
<hr>
<h2 id="learning-temporal-pose-estimation-from-sparsely-labeled-videos">Learning Temporal Pose Estimation from Sparsely-Labeled Videos</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-44">abstract</h3>
<p>Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames
---a labeled Frame A and an unlabeled Frame B
---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7% mAP vs 83.8% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows our system to achieve state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: <a href="https://github.com/facebookresearch/PoseWarper">https://github.com/facebookresearch/PoseWarper</a>.</p>
<hr>
<h2 id="cross-view-fusion-for-3d-human-pose-estimation">Cross view fusion for 3D human pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-45">abstract</h3>
<p>We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already benefits from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm).</p>
<hr>
<h2 id="dynamic-kernel-distillation-for-efficient-pose-estimation-in-videos">Dynamic kernel distillation for efficient pose estimation in videos</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-46">abstract</h3>
<p>Existing video-based human pose estimation methods extensively apply large networks onto every frame in the video to localize body joints, which suffer high computational cost and hardly meet the low-latency requirement in realistic applications. To address this issue, we propose a novel Dynamic Kernel Distillation (DKD) model to facilitate small networks for estimating human poses in videos, thus significantly lifting the efficiency. In particular, DKD introduces a light-weight distillator to online distill pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame, which can be efficiently computed via simple convolution. In this way, DKD fast transfers pose knowledge from one frame to provide compact guidance for body joint localization in the following frame, which enables utilization of small networks in video-based pose estimation. To facilitate the training process, DKD exploits a temporally adversarial training strategy that introduces a temporal discriminator to help generate temporally coherent pose kernels and pose estimation results within a long range. Experiments on Penn Action and Sub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically, 10x flops reduction and 2x speedup over previous best model, and its state-of-the-art accuracy.</p>
<hr>
<h2 id="denserac-joint-3d-pose-and-shape-estimation-by-dense-render-and-compare">DenseRaC: Joint 3D pose and shape estimation by dense render-and-compare</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-47">abstract</h3>
<p>We present DenseRaC, a novel end-to-end framework for jointly estimating 3D human pose and body shape from a monocular RGB image. Our two-step framework takes the body pixel-to-surface correspondence map (i.e., IUV map) as proxy representation and then performs estimation of parameterized human pose and shape. Specifically, given an estimated IUV map, we develop a deep neural network optimizing 3D body reconstruction losses and further integrating a render-and-compare scheme to minimize differences between the input and the rendered output, i.e., dense body landmarks, body part masks, and adversarial priors. To boost learning, we further construct a large-scale synthetic dataset (MOCA) utilizing web-crawled Mocap sequences, 3D scans and animations. The generated data covers diversified camera views, human actions and body shapes, and is paired with full ground truth. Our model jointly learns to represent the 3D human body from hybrid datasets, mitigating the problem of unpaired training data. Our experiments show that DenseRaC obtains superior performance against state of the art on public benchmarks of various human-related tasks.</p>
<hr>
<h2 id="shape-aware-human-pose-and-shape-reconstruction-using-multi-view-images">Shape-aware human pose and shape reconstruction using multi-view images</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-48">abstract</h3>
<p>We propose a scalable neural network framework to reconstruct the 3D mesh of a human body from multi-view images, in the subspace of the SMPL model. Use of multi-view images can significantly reduce the projection ambiguity of the problem, increasing the reconstruction accuracy of the 3D human body under clothing. Our experiments show that this method benefits from the synthetic dataset generated from our pipeline since it has good flexibility of variable control and can provide ground-truth for validation. Our method outperforms existing methods on real-world images, especially on shape estimations.</p>
<hr>
<h2 id="monocular-3d-human-pose-estimation-by-generation-and-ordinal-ranking">Monocular 3D human pose estimation by generation and ordinal ranking</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-49">abstract</h3>
<p>Monocular 3D human-pose estimation from static images is a challenging problem, due to the curse of dimensionality and the ill-posed nature of lifting 2D-to-3D. In this paper, we propose a Deep Conditional Variational Autoencoder based model that synthesizes diverse anatomically plausible 3D-pose samples conditioned on the estimated 2D-pose. We show that CVAE-based 3D-pose sample set is consistent with the 2D-pose and helps tackling the inherent ambiguity in 2D-to-3D lifting. We propose two strategies for obtaining the final 3D pose- (a) depth-ordering/ordinal relations to score and weight-average the candidate 3D-poses, referred to as OrdinalScore, and (b) with supervision from an Oracle. We report close to state-of-the-art results on two benchmark datasets using OrdinalScore, and state-of-the-art results using the Oracle. We also show that our pipeline yields competitive results without paired image-to-3D annotations. The training and evaluation code is available at <a href="https://github.com/ssfootball04/generative-pose">https://github.com/ssfootball04/generative-pose</a>.</p>
<hr>
<h2 id="single-stage-multi-person-pose-machines">Single-stage multi-person pose machines</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-50">abstract</h3>
<p>Multi-person pose estimation is a challenging problem. Existing methods are mostly two-stage based-one stage for proposal generation and the other for allocating poses to corresponding persons. However, such two-stage methods generally suffer low efficiency. In this work, we present the first emph{single-stage} model, Single-stage multi-person Pose Machine (SPM), to simplify the pipeline and lift the efficiency for multi-person pose estimation. To achieve this, we propose a novel Structured Pose Representation (SPR) that unifies person instance and body joint position representations. Based on SPR, we develop the SPM model that can directly predict structured poses for multiple persons in a single stage, and thus offer a more compact pipeline and attractive efficiency advantage over two-stage methods. In particular, SPR introduces the root joints to indicate different person instances and human body joint positions are encoded into their displacements w.r.t.∼the roots. To better predict long-range displacements for some joints, SPR is further extended to hierarchical representations. Based on SPR, SPM can efficiently perform multi-person poses estimation by simultaneously predicting root joints (location of instances) and body joint displacements via CNNs. Moreover, to demonstrate the generality of SPM, we also apply it to multi-person 3D pose estimation. Comprehensive experiments on benchmarks MPII, extended PASCAL-Person-Part, MSCOCO and CMU Panoptic clearly demonstrate the state-of-the-art efficiency of SPM for multi-person 2D/3D pose estimation, together with outstanding accuracy.</p>
<hr>
<h2 id="learning-to-reconstruct-3d-human-pose-and-shape-via-model-fitting-in-the-loop">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-51">abstract</h3>
<p>Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at <a href="https://seas.upenn.edu/%E2%88%BCnkolot/projects/spin">https://seas.upenn.edu/∼nkolot/projects/spin</a>.</p>
<hr>
<h2 id="higherhrnet-scale-aware-representation-learning-for-bottom-up-human-pose-estimation">HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-52">abstract</h3>
<p>Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene. The code and models are available at <a href="https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation">https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation</a>.</p>
<hr>
<h2 id="distribution-aware-coordinate-representation-for-human-pose-estimation">Distribution-Aware Coordinate Representation for Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-53">abstract</h3>
<p>While being the de facto standard coordinate representation in human pose estimation, heatmap is never systematically investigated in the literature, to our best knowledge. This work fills this gap by studying the coordinate representation with a particular focus on the heatmap. Interestingly, we found that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for human pose estimation performance, which nevertheless was not recognised before. In light of the discovered importance, we further probe the design limitations of the standard coordinate decoding method widely used by existing methods, and propose a more principled distribution-aware decoding method. Meanwhile, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating accurate heatmap distributions for unbiased model training. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoint (DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves the performance of a variety of state-of-the-art human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO, consistently validating the usefulness and effectiveness of our novel coordinate representation idea.</p>
<hr>
<h2 id="multi-path-learning-for-object-pose-estimation-across-domains">Multi-path Learning for Object Pose Estimation Across Domains</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-54">abstract</h3>
<p>We introduce a scalable approach for object pose estimation trained on simulated RGB views of multiple 3D models together. We learn an encoding of object views that does not only describe an implicit orientation of all objects seen during training, but can also relate views of untrained objects. Our single-encoder-multi-decoder network is trained using a technique we denote &quot;multi-path learning&quot;: While the encoder is shared by all objects, each decoder only reconstructs views of a single object. Consequently, views of different instances do not have to be separated in the latent space and can share common features. The resulting encoder generalizes well from synthetic to real data and across various instances, categories, model types and datasets. We systematically investigate the learned encodings, their generalization, and iterative refinement strategies on the ModelNet40 and T-LESS dataset. Despite training jointly on multiple objects, our 6D Object Detection pipeline achieves state-of-the-art results on T-LESS at much lower runtimes than competing approaches.</p>
<hr>
<h2 id="graph-embedded-pose-clustering-for-anomaly-detection">Graph Embedded Pose Clustering for Anomaly Detection</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-55">abstract</h3>
<p>We propose a new method for anomaly detection of human actions. Our method works directly on human pose graphs that can be computed from an input video sequence. This makes the analysis independent of nuisance parameters such as viewpoint or illumination. We map these graphs to a latent space and cluster them. Each action is then represented by its soft-assignment to each of the clusters. This gives a kind of &quot;bag of words&quot; representation to the data, where every action is represented by its similarity to a group of base action-words. Then, we use a Dirichlet process based mixture, that is useful for handling proportional data such as our soft-assignment vectors, to determine if an action is normal or not. We evaluate our method on two types of data sets. The first is a fine-grained anomaly detection data set (e.g. ShanghaiTech) where we wish to detect unusual variations of some action. The second is a coarse-grained anomaly detection data set (e.g., a Kinetics-based data set) where few actions are considered normal, and every other action should be considered abnormal. Extensive experiments on the benchmarks show that our method performs considerably better than other state of the art methods.</p>
<hr>
<h2 id="just-go-with-the-flow-self-supervised-scene-flow-estimation">Just Go with the Flow: Self-Supervised Scene Flow Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-56">abstract</h3>
<p>When interacting with highly dynamic environments, scene flow allows autonomous systems to reason about the non-rigid motion of multiple independent objects. This is of particular interest in the field of autonomous driving, in which many cars, people, bicycles, and other objects need to be accurately tracked. Current state-of-the-art methods require annotated scene flow data from autonomous driving scenes to train scene flow networks with supervised learning. As an alternative, we present a method of training scene flow that uses two self-supervised losses, based on nearest neighbors and cycle consistency. These self-supervised losses allow us to train our method on large unlabeled autonomous driving datasets; the resulting method matches current state-of-the-art supervised performance using no real world annotations and exceeds state-of-the-art performance when combining our self-supervised approach with supervised learning on a smaller labeled dataset.</p>
<hr>
<h2 id="extreme-relative-pose-network-under-hybrid-representations">Extreme Relative Pose Network under Hybrid Representations</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-57">abstract</h3>
<p>In this paper, we introduce a novel RGB-D based relative pose estimation approach that is suitable for small-overlapping or non-overlapping scans and can output multiple relative poses. Our method performs scene completion and matches the completed scans. However, instead of using a fixed representation for completion, the key idea is to utilize hybrid representations that combine 360-image, 2D image-based layout, and planar patches. This approach offers adaptively feature representations for relative pose estimation. Besides, we introduce a global-2-local matching procedure, which utilizes initial relative poses obtained during the global phase to detect and then integrate geometric relations for pose refinement. Experimental results justify the potential of this approach across a wide range of benchmark datasets. For example, on ScanNet, the rotation translation errors of the top-1/top-5 predictions of our approach are 28.6/0.90m and 16.8/0.76m, respectively. Our approach also considerably boosts the performance of multi-scan reconstruction in few-view reconstruction settings.</p>
<hr>
<h2 id="the-devil-is-in-the-details-delving-into-unbiased-data-processing-for-human-pose-estimation">The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-58">abstract</h3>
<p>Recently, the leading performance of human pose estimation is dominated by top-down methods. Being a fundamental component in training and inference, data processing has not been systematically considered in pose estimation community, to the best of our knowledge. In this paper, we focus on this problem and find that the devil of top-down pose estimator is in the biased data processing. Specifically, by investigating the standard data processing in state-of-the-art approaches mainly including data transformation and encoding-decoding, we find that the results obtained by common flipping strategy are unaligned with the original ones in inference. Moreover, there is statistical error in standard encoding-decoding during both training and inference. Two problems couple together and significantly degrade the pose estimation performance. Based on quantitative analyses, we then formulate a principled way to tackle this dilemma. Data is processed based on unit length instead of pixel, and an offset-based strategy is adopted to perform encoding-decoding. The Unbiased Data Processing (UDP) for human pose estimation can be achieved by combining the two together. UDP not only boosts the performance of existing methods by a large margin but also plays a important role in result reproducing and future exploration. As a model-agnostic approach, UDP promotes SimpleBaseline-ResNet-50-256x192 by 1.5 AP (70.2 to 71.7) and HRNet-W32-256x192 by 1.7 AP (73.5 to 75.2) on COCO test-dev set. The HRNet-W48-384x288 equipped with UDP achieves 76.5 AP and sets a new state-of-the-art for human pose estimation. The code will be released.</p>
<hr>
<h2 id="factorized-higher-order-cnns-with-an-application-to-spatio-temporal-emotion-estimation">Factorized Higher-Order CNNs with an Application to Spatio-Temporal Emotion Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-59">abstract</h3>
<p>Training deep neural networks with spatio-temporal (i.e., 3D) or multidimensional convolutions of higher-order is computationally challenging due to millions of unknown parameters across dozens of layers. To alleviate this, one approach is to apply low-rank tensor decompositions to convolution kernels in order to compress the network and reduce its number of parameters. Alternatively, new convolutional blocks, such as MobileNet, can be directly designed for efficiency. In this paper, we unify these two approaches by proposing a tensor factorization framework for efficient multidimensional (separable) convolutions of higher-order. Interestingly, the proposed framework enables a novel higher-order transduction, allowing to train a network on a given domain (e.g., 2D images or N-dimensional data in general) and using transduction to generalize to higher-order data such as videos (or (N+K)-dimensional data in general), capturing for instance temporal dynamics while preserving the learnt spatial information. We apply the proposed methodology, coined CP-Higher-Order Convolution (HO-CPConv), to spatio-temporal facial emotion analysis. Most existing facial affect models focus on static imagery and discard all temporal information. This is due to the above-mentioned burden of training 3D convolutional nets and the lack of large bodies of video data annotated by experts. We address both issues with our proposed framework. Initial training is first done on static imagery before using transduction to generalize to the temporal domain. We demonstrate superior performance on three challenging large scale affect estimation datasets, AffectNet, SEWA, and AFEW-VA.</p>
<hr>
<h2 id="multiview-consistent-semi-supervised-learning-for-3d-human-pose-estimation">Multiview-Consistent Semi-Supervised Learning for 3D Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-60">abstract</h3>
<p>The best performing methods for 3D human pose estimation from monocular images require large amounts of in-the-wild 2D and controlled 3D pose annotated datasets which are costly and require sophisticated systems to acquire. To reduce this annotation dependency, we propose Multiview-Consistent Semi Supervised Learning (MCSS) framework that utilizes similarity in pose information from unannotated, uncalibrated but synchronized multi-view videos of human motions as additional weak supervision signal to guide 3D human pose regression. Our framework applies hard-negative mining based on temporal relations in multi-view videos to arrive at a multi-view consistent pose embedding. When jointly trained with limited 3D pose annotations, our approach improves the baseline by 25% and state-of-the-art by 8.7%, whilst using substantially smaller networks. Lastly, but importantly, we demonstrate the advantages of the learned embedding and establish view-invariant pose retrieval benchmarks on two popular, publicly available multi-view human pose datasets, Human 3.6M and MPI-INF-3DHP, to facilitate future research.</p>
<hr>
<h2 id="self-supervised-learning-of-3d-human-pose-using-multi-view-geometry">Self-supervised learning of 3D human pose using multi-view geometry</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-61">abstract</h3>
<p>Training accurate 3D human pose estimators requires large amount of 3D ground-truth data which is costly to collect. Various weakly or self supervised pose estimation methods have been proposed due to lack of 3D data. Nevertheless, these methods, in addition to 2D ground-truth poses, require either additional supervision in various forms (e.g. unpaired 3D ground truth data, a small subset of labels) or the camera parameters in multiview settings. To address these problems, we present EpipolarPose, a self-supervised learning method for 3D human pose estimation, which does not need any 3D ground-truth data or camera extrinsics. During training, EpipolarPose estimates 2D poses from multi-view images, and then, utilizes epipolar geometry to obtain a 3D pose and camera geometry which are subsequently used to train a 3D pose estimator. We demonstrate the effectiveness of our approach on standard benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) where we set the new state-of-the-art among weakly/self-supervised methods. Furthermore, we propose a new performance measure Pose Structure Score (PSS) which is a scale invariant, structure aware measure to evaluate the structural plausibility of a pose with respect to its ground truth. Code and pretrained models are available at <a href="https://github.com/mkocabas/EpipolarPose">https://github.com/mkocabas/EpipolarPose</a>.</p>
<hr>
<h2 id="vibe-video-inference-for-human-body-pose-and-shape-estimation">VIBE: Video Inference for Human Body Pose and Shape Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-62">abstract</h3>
<p>Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose Video Inference for Body Pose and Shape Estimation (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at <a href="https://github.com/mkocabas/VIBE">https://github.com/mkocabas/VIBE</a>.</p>
<hr>
<h2 id="multi-person-pose-estimation-with-enhanced-channel-wise-and-spatial-information">Multi-person pose estimation with enhanced channel-wise and spatial information</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-63">abstract</h3>
<p>Multi-person pose estimation is an important but challenging problem in computer vision. Although current approaches have achieved significant progress by fusing the multi-scale feature maps, they pay little attention to enhancing the channel-wise and spatial information of the feature maps. In this paper, we propose two novel modules to perform the enhancement of the information for the multi-person pose estimation. First, a Channel Shuffle Module (CSM) is proposed to adopt the channel shuffle operation on the feature maps with different levels, promoting cross-channel information communication among the pyramid feature maps. Second, a Spatial, Channel-wise Attention Residual Bottleneck (SCARB) is designed to boost the original residual unit with attention mechanism, adaptively highlighting the information of the feature maps both in the spatial and channel-wise context. The effectiveness of our proposed modules is evaluated on the COCO keypoint benchmark, and experimental results show that our approach achieves the state-of-the-art results.</p>
<hr>
<h2 id="deep-high-resolution-representation-learning-for-human-pose-estimation">Deep high-resolution representation learning for human pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-64">abstract</h3>
<p>In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: The COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at <a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</a>.</p>
<hr>
<h2 id="does-learning-specific-features-for-related-parts-help-human-pose-estimation">Does learning specific features for related parts help human pose estimation?</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-65">abstract</h3>
<p>Human pose estimation (HPE) is inherently a homogeneous multi-task learning problem, with the localization of each body part as a different task. Recent HPE approaches universally learn a shared representation for all parts, from which their locations are linearly regressed. However, our statistical analysis indicates not all parts are related to each other. As a result, such a sharing mechanism can lead to negative transfer and deteriorate the performance. This potential issue drives us to raise an interesting question. Can we identify related parts and learn specific features for them to improve pose estimation? Since unrelated tasks no longer share a high-level representation, we expect to avoid the adverse effect of negative transfer. In addition, more explicit structural knowledge, e.g., ankles and knees are highly related, is incorporated into the model, which helps resolve ambiguities in HPE. To answer this question, we first propose a data-driven approach to group related parts based on how much information they share. Then a part-based branching network (PBN) is introduced to learn representations specific to each part group. We further present a multi-stage version of this network to repeatedly refine intermediate features and pose estimates. Ablation experiments indicate learning specific features significantly improves the localization of occluded parts and thus benefits HPE. Our approach also outperforms all state-of-the-art methods on two benchmark datasets, with an outstanding advantage when occlusion occurs.</p>
<hr>
<h2 id="3d-human-pose-estimation-in-video-with-temporal-convolutions-and-semi-supervised-training">3D human pose estimation in video with temporal convolutions and semi-supervised training</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-66">abstract</h3>
<p>In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at <a href="https://github.com/facebookresearch/VideoPose3D">https://github.com/facebookresearch/VideoPose3D</a>.</p>
<hr>
<h2 id="semantic-graph-convolutional-networks-for-3d-human-pose-regression">Semantic graph convolutional networks for 3D human pose regression</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-67">abstract</h3>
<p>In this paper, we study the problem of learning Graph Convolutional Networks (GCNs) for regression. Current architectures of GCNs are limited to the small receptive field of convolution filters and shared transformation matrix for each node. To address these limitations, we propose Semantic Graph Convolutional Networks (SemGCN), a novel neural network architecture that operates on regression tasks with graph-structured data. SemGCN learns to capture semantic information such as local and global node relationships, which is not explicitly represented in the graph. These semantic relationships can be learned through end-to-end training from the ground truth without additional supervision or hand-crafted rules. We further investigate applying SemGCN to 3D human pose regression. Our formulation is intuitive and sufficient since both 2D and 3D human poses can be represented as a structured graph encoding the relationships between joints in the skeleton of a human body. We carry out comprehensive studies to validate our method. The results prove that SemGCN outperforms state of the art while using 90% fewer parameters.</p>
<hr>
<h2 id="ige-net-inverse-graphics-energy-networks-for-human-pose-estimation-and-single-view-reconstruction">IGE-net: Inverse graphics energy networks for human pose estimation and single-view reconstruction</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-68">abstract</h3>
<p>Inferring 3D scene information from 2D observations is an open problem in computer vision. We propose using a deep-learning based energy minimization framework to learn a consistency measure between 2D observations and a proposed world model, and demonstrate that this framework can be trained end-to-end to produce consistent and realistic inferences. We evaluate the framework on human pose estimation and voxel-based object reconstruction benchmarks and show competitive results can be achieved with relatively shallow networks with drastically fewer learned parameters and floating point operations than conventional deep-learning approaches.</p>
<hr>
<h2 id="pa3d-pose-action-3d-machine-for-video-recognition">PA3D: Pose-action 3D machine for video recognition</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-69">abstract</h3>
<p>Recent studies have witnessed the successes of using 3D CNNs for video action recognition. However, most 3D models are built upon RGB and optical flow streams, which may not fully exploit pose dynamics, i.e., an important cue of modeling human actions. To fill this gap, we propose a concise Pose-Action 3D Machine (PA3D), which can effectively encode multiple pose modalities within a unified 3D framework, and consequently learn spatio-temporal pose representations for action recognition. More specifically, we introduce a novel temporal pose convolution to aggregate spatial poses over frames. Unlike the classical temporal convolution, our operation can explicitly learn the pose motions that are discriminative to recognize human actions. Extensive experiments on three popular benchmarks (i.e., JHMDB, HMDB, and Charades) show that, PA3D outperforms the recent pose-based approaches. Furthermore, PA3D is highly complementary to the recent 3D CNNs, e.g., I3D. Multi-stream fusion achieves the state-of-the-art performance on all evaluated data sets.</p>
<hr>
<h2 id="generating-multiple-hypotheses-for-3d-human-pose-estimation-with-mixture-density-network">Generating multiple hypotheses for 3D human pose estimation with mixture density network</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-70">abstract</h3>
<p>3D human pose estimation from a monocular image or 2D joints is an ill-posed problem because of depth ambiguity and occluded joints. We argue that 3D human pose estimation from a monocular input is an inverse problem where multiple feasible solutions can exist. In this paper, we propose a novel approach to generate multiple feasible hypotheses of the 3D pose from 2D joints. In contrast to existing deep learning approaches which minimize a mean square error based on an unimodal Gaussian distribution, our method is able to generate multiple feasible hypotheses of 3D pose based on a multimodal mixture density networks. Our experiments show that the 3D poses estimated by our approach from an input of 2D joints are consistent in 2D reprojections, which supports our argument that multiple solutions exist for the 2D-to-3D inverse problem. Furthermore, we show state-of-the-art performance on the Human3.6M dataset in both best hypothesis and multi-view settings, and we demonstrate the generalization capacity of our model by testing on the MPII and MPI-INF-3DHP datasets. Our code is available at the project website.</p>
<hr>
<h2 id="unsupervised-3d-pose-estimation-with-geometric-self-supervision">Unsupervised 3D pose estimation with geometric self-supervision</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-71">abstract</h3>
<p>We present an unsupervised learning approach to re-cover 3D human pose from 2D skeletal joints extracted from a single image. Our method does not require any multi-view image data, 3D skeletons, correspondences between 2D-3D points, or use previously learned 3D priors during training. A lifting network accepts 2D landmarks as inputs and generates a corresponding 3D skeleton estimate. Dur-ing training, the recovered 3D skeleton is reprojected on random camera viewpoints to generate new 'synthetic' 2D poses. By lifting the synthetic 2D poses back to 3D and re-projecting them in the original camera view, we can de-fine self-consistency loss both in 3D and in 2D. The training can thus be self supervised by exploiting the geometric self-consistency of the lift-reproject-lift process. We show that self-consistency alone is not sufficient to generate realistic skeletons, however adding a 2D pose discriminator enables the lifter to output valid 3D poses. Additionally, to learn from 2D poses 'in the wild', we train an unsupervised 2D domain adapter network to allow for an expansion of 2D data. This improves results and demonstrates the useful-ness of 2D pose data for unsupervised 3D lifting. Results on Human3.6M dataset for 3D human pose estimation demon-strate that our approach improves upon the previous un-supervised methods by 30% and outperforms many weakly supervised approaches that explicitly use 3D data.</p>
<hr>
<h2 id="posefix-model-agnostic-general-human-pose-refinement-network">Posefix: Model-agnostic general human pose refinement network</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-72">abstract</h3>
<p>Multi-person pose estimation from a 2D image is an essential technique for human behavior understanding. In this paper, we propose a human pose refinement network that estimates a refined pose from a tuple of an input image and input pose. The pose refinement was performed mainly through an end-to-end trainable multi-stage architecture in previous methods. However, they are highly dependent on pose estimation models and require careful model design. By contrast, we propose a model-agnostic pose refinement method. According to a recent study, state-of-the-art 2D human pose estimation methods have similar error distributions. We use this error statistics as prior information to generate synthetic poses and use the synthesized poses to train our model. In the testing stage, pose estimation results of any other methods can be input to the proposed method. Moreover, the proposed model does not require code or knowledge about other methods, which allows it to be easily used in the post-processing step. We show that the proposed approach achieves better performance than the conventional multi-stage refinement models and consistently improves the performance of various state-of-the-art pose estimation methods on the commonly used benchmark. The code is available in footnote{url{<a href="https://github.com/mks0601/PoseFix">https://github.com/mks0601/PoseFix</a> RELEASE}.</p>
<hr>
<h2 id="deepfashion2-a-versatile-benchmark-for-detection-pose-estimation-segmentation-and-re-identification-of-clothing-images">Deepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-73">abstract</h3>
<p>Understanding fashion images has been advanced by benchmarks with rich annotations such as DeepFashion, whose labels include clothing categories, landmarks, and consumer-commercial image pairs. However, DeepFashion has nonnegligible issues such as single clothing-item per image, sparse landmarks (4∼8 only), and no per-pixel masks, making it had significant gap from real-world scenarios. We fill in the gap by presenting DeepFashion2 to address these issues. It is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, view-point, occlusion, bounding box, dense landmarks (e.g. 39 for 'long sleeve outwear' and 15 for 'vest'), and masks. There are also 873K Commercial-Consumer clothes pairs. The annotations of DeepFashion2 are much larger than its counterparts such as 8× of FashionAI Global Challenge. A strong baseline is proposed, called Match R-CNN, which builds upon Mask R-CNN to solve the above four tasks in an end-to-end manner. Extensive evaluations are conducted with different criterions in Deep-Fashion2. DeepFashion2 Dataset will be released at : <a href="https://github.com/switchablenorms/DeepFashion2">https://github.com/switchablenorms/DeepFashion2</a></p>
<hr>
<h2 id="pifpaf-composite-fields-for-human-pose-estimation">PifPaf: Composite fields for human pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-74">abstract</h3>
<p>We propose a new bottom-up method for multi-person 2D human pose estimation that is particularly well suited for urban mobility such as self-driving cars and delivery robots. The new method, PifPaf, uses a Part Intensity Field (PIF) to localize body parts and a Part Association Field (PAF) to associate body parts with each other to form full human poses. Our method outperforms previous methods at low resolution and in crowded, cluttered and occluded scenes thanks to (i) our new composite field PAF encoding fine-grained information and (ii) the choice of Laplace loss for regressions which incorporates a notion of uncertainty. Our architecture is based on a fully convolutional, single-shot, box-free design. We perform on par with the existing state-of-the-art bottom-up method on the standard COCO keypoint task and produce state-of-the-art results on a modified COCO keypoint task for the transportation domain.</p>
<hr>
<h2 id="exploiting-spatial-temporal-relationships-for-3d-pose-estimation-via-graph-convolutional-networks">Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-75">abstract</h3>
<p>Despite great progress in 3D pose estimation from single-view images or videos, it remains a challenging task due to the substantial depth ambiguity and severe self-occlusions. Motivated by the effectiveness of incorporating spatial dependencies and temporal consistencies to alleviate these issues, we propose a novel graph-based method to tackle the problem of 3D human body and 3D hand pose estimation from a short sequence of 2D joint detections. Particularly, domain knowledge about the human hand (body) configurations is explicitly incorporated into the graph convolutional operations to meet the specific demand of the 3D pose estimation. Furthermore, we introduce a local-to-global network architecture, which is capable of learning multi-scale features for the graph-based representations. We evaluate the proposed method on challenging benchmark datasets for both 3D hand pose estimation and 3D body pose estimation. Experimental results show that our method achieves state-of-the-art performance on both tasks.</p>
<hr>
<h2 id="distill-knowledge-from-nrsfm-for-weakly-supervised-3d-pose-learning">Distill knowledge from NRSfM for weakly supervised 3D pose learning</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-76">abstract</h3>
<p>We propose to learn a 3D pose estimator by distilling knowledge from Non-Rigid Structure from Motion (NRSfM). Our method uses solely 2D landmark annotations. No 3D data, multi-view/temporal footage, or object specific prior is required. This alleviates the data bottleneck, which is one of the major concern for supervised methods. The challenge for using NRSfM as teacher is that they often make poor depth reconstruction when the 2D projections have strong ambiguity. Directly using those wrong depth as hard target would negatively impact the student. Instead, we propose a novel loss that ties depth prediction to the cost function used in NRSfM. This gives the student pose estimator freedom to reduce depth error by associating with image features. Validated on H3.6M dataset, our learned 3D pose estimation network achieves more accurate reconstruction compared to NRSfM methods. It also outperforms other weakly supervised methods, in spite of using significantly less supervision.</p>
<hr>
<h2 id="occlusion-aware-networks-for-3d-human-pose-estimation-in-video">Occlusion-aware networks for 3D human pose estimation in video</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-77">abstract</h3>
<p>Occlusion is a key problem in 3D human pose estimation from a monocular video. To address this problem, we introduce an occlusion-aware deep-learning framework. By employing estimated 2D confidence heatmaps of keypoints and an optical-flow consistency constraint, we filter out the unreliable estimations of occluded keypoints. When occlusion occurs, we have incomplete 2D keypoints and feed them to our 2D and 3D temporal convolutional networks (2D and 3D TCNs) that enforce temporal smoothness to produce a complete 3D pose. By using incomplete 2D keypoints, instead of complete but incorrect ones, our networks are less affected by the error-prone estimations of occluded keypoints. Training the occlusion-aware 3D TCN requires pairs of a 3D pose and a 2D pose with occlusion labels. As no such a dataset is available, we introduce a ''Cylinder Man Model'' to approximate the occupation of body parts in 3D space. By projecting the model onto a 2D plane in different viewing angles, we obtain and label the occluded keypoints, providing us plenty of training data. In addition, we use this model to create a pose regularization constraint, preferring the 2D estimations of unreliable keypoints to be occluded. Our method outperforms state-of-the-art methods on Human 3.6M and HumanEva-I datasets.</p>
<hr>
<h2 id="moulding-humans-non-parametric-3d-human-shape-estimation-from-single-images">Moulding humans: Non-parametric 3D human shape estimation from single images</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-78">abstract</h3>
<p>In this paper, we tackle the problem of 3D human shape estimation from single RGB images. While the recent progress in convolutional neural networks has allowed impressive results for 3D human pose estimation, estimating the full 3D shape of a person is still an open issue. Model-based approaches can output precise meshes of naked under-cloth human bodies but fail to estimate details and un-modelled elements such as hair or clothing. On the other hand, non-parametric volumetric approaches can potentially estimate complete shapes but, in practice, they are limited by the resolution of the output grid and cannot produce detailed estimates. In this work, we propose a non-parametric approach that employs a double depth map to represent the 3D shape of a person: A visible depth map and a ''hidden'' depth map are estimated and combined, to reconstruct the human 3D shape as done with a ''mould''. This representation through 2D depth maps allows a higher resolution output with a much lower dimension than voxel-based volumetric representations. Additionally, our fully derivable depth-based model allows us to efficiently incorporate a discriminator in an adversarial fashion to improve the accuracy and ''humanness'' of the 3D output. We train and quantitatively validate our approach on SURREAL and on 3D-HUMANS, a new photorealistic dataset made of semi-synthetic in-house videos annotated with 3D ground truth surfaces.</p>
<hr>
<h2 id="optimizing-network-structure-for-3d-human-pose-estimation">Optimizing network structure for 3D human pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-79">abstract</h3>
<p>A human pose is naturally represented as a graph where the joints are the nodes and the bones are the edges. So it is natural to apply Graph Convolutional Network (GCN) to estimate 3D poses from 2D poses. In this work, we propose a generic formulation where both GCN and Fully Connected Network (FCN) are its special cases. From this formulation, we discover that GCN has limited representation power when used for estimating 3D poses. We overcome the limitation by introducing Locally Connected Network (LCN) which is naturally implemented by this generic formulation. It notably improves the representation capability over GCN. In addition, since every joint is only connected to a few joints in its neighborhood, it has strong generalization power. The experiments on public datasets show it: (1) outperforms the state-of-the-arts; (2) is less data hungry than alternative models; (3) generalizes well to unseen actions and datasets.</p>
<hr>
<h2 id="gp2c-geometric-projection-parameter-consensus-for-joint-3d-pose-and-focal-length-estimation-in-the-wild">GP2C: Geometric projection parameter consensus for joint 3D pose and focal length estimation in the wild</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-80">abstract</h3>
<p>We present a joint 3D pose and focal length estimation approach for object categories in the wild. In contrast to previous methods that predict 3D poses independently of the focal length or assume a constant focal length, we explicitly estimate and integrate the focal length into the 3D pose estimation. For this purpose, we combine deep learning techniques and geometric algorithms in a two-stage approach: First, we estimate an initial focal length and establish 2D-3D correspondences from a single RGB image using a deep network. Second, we recover 3D poses and refine the focal length by minimizing the reprojection error of the predicted correspondences. In this way, we exploit the geometric prior given by the focal length for 3D pose estimation. This results in two advantages: First, we achieve significantly improved 3D translation and 3D pose accuracy compared to existing methods. Second, our approach finds a geometric consensus between the individual projection parameters, which is required for precise 2D-3D alignment. We evaluate our proposed approach on three challenging real-world datasets (Pix3D, Comp, and Stanford) with different object categories and significantly outperform the state-of-the-art by up to 20% absolute in multiple different metrics.</p>
<hr>
<h2 id="a2j-anchor-to-joint-regression-network-for-3d-articulated-pose-estimation-from-a-single-depth-image">A2J: Anchor-to-joint regression network for 3D articulated pose estimation from a single depth image</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-81">abstract</h3>
<p>For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-to-Joint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet- 50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J's superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU.</p>
<hr>
<h2 id="xr-egopose-egocentric-3d-human-pose-from-an-hmd-camera">XR-EgoPose: Egocentric 3D human pose from an HMD camera</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-82">abstract</h3>
<p>We present a new solution to egocentric 3D body pose estimation from monocular images captured from a downward looking fish-eye camera installed on the rim of a head mounted virtual reality device. This unusual viewpoint, just 2 cm.∼away from the user's face, leads to images with unique visual appearance, characterized by severe self-occlusions and strong perspective distortions that result in a drastic difference in resolution between lower and upper body. Our contribution is two-fold. Firstly, we propose a new encoder-decoder architecture with a novel dual branch decoder designed specifically to account for the varying uncertainty in the 2D joint locations. Our quantitative evaluation, both on synthetic and real-world datasets, shows that our strategy leads to substantial improvements in accuracy over state of the art egocentric pose estimation approaches. Our second contribution is a new large-scale photorealistic synthetic dataset - xR-EgoPose - offering 383K frames of high quality renderings of people with a diversity of skin tones, body shapes, clothing, in a variety of backgrounds and lighting conditions, performing a range of actions. Our experiments show that the high variability in our new synthetic training corpus leads to good generalization to real world footage and to state of the art results on real world datasets with ground truth. Moreover, an evaluation on the Human3.6M benchmark shows that the performance of our method is on par with top performing approaches on the more classic problem of 3D human pose from a third person viewpoint.</p>
<hr>
<h2 id="panoptic-studio-a-massively-multiview-system-for-social-interaction-capture">Panoptic Studio: A Massively Multiview System for Social Interaction Capture</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-83">abstract</h3>
<p>Panoptic Studio: A Massively Multiview System for Social Interaction Capture</p>
<hr>
<h2 id="end-to-end-learning-for-graph-decomposition">End-to-end learning for graph decomposition</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-84">abstract</h3>
<p>Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation.</p>
<hr>
<h2 id="fast-and-robust-multi-person-3d-pose-estimation-from-multiple-views">Fast and robust multi-person 3D pose estimation from multiple views</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-85">abstract</h3>
<p>This paper addresses the problem of 3D pose estimation for multiple people in a few calibrated camera views. The main challenge of this problem is to find the cross-view correspondences among noisy and incomplete 2D pose predictions. Most previous methods address this challenge by directly reasoning in 3D using a pictorial structure model, which is inefficient due to the huge state space. We propose a fast and robust approach to solve this problem. Our key idea is to use a multi-way matching algorithm to cluster the detected 2D poses in all views. Each resulting cluster encodes 2D poses of the same person across different views and consistent correspondences across the keypoints, from which the 3D pose of each person can be effectively inferred. The proposed convex optimization based multi-way matching algorithm is efficient and robust against missing and false detections, without knowing the number of people in the scene. Moreover, we propose to combine geometric and appearance cues for cross-view matching. The proposed approach achieves significant performance gains from the state-of-the-art (96.3% vs. 90.6% and 96.9% vs. 88% on the Campus and Shelf datasets, respectively), while being efficient for real-time applications.</p>
<hr>
<h1 id="5-papers-in-2020">5. Papers in 2020</h1>
<h2 id="lightweight-multi-view-3d-pose-estimation-through-camera-disentangled-representation">Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-86">abstract</h3>
<p>We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate per-view 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance.</p>
<hr>
<h2 id="hope-net-a-graph-based-model-for-hand-object-pose-estimation">HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-87">abstract</h3>
<p>Hand-object pose estimation (HOPE) aims to jointly detect the poses of both a hand and of a held object. In this paper, we propose a lightweight model called HOPE-Net which jointly estimates hand and object pose in 2D and 3D in real-time. Our network uses a cascade of two adaptive graph convolutional neural networks, one to estimate 2D coordinates of the hand joints and object corners, followed by another to convert 2D coordinates to 3D. Our experiments show that through end-to-end training of the full network, we achieve better accuracy for both the 2D and 3D coordinate estimation problems. The proposed 2D to 3D graph convolution-based model could be applied to other 3D landmark detection problems, where it is possible to first predict the 2D keypoints and then transform them to 3D.</p>
<hr>
<h2 id="neural-pose-transfer-by-spatially-adaptive-instance-normalization">Neural Pose Transfer by Spatially Adaptive Instance Normalization</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-88">abstract</h3>
<p>Pose transfer has been studied for decades, in which the pose of a source mesh is applied to a target mesh. Particularly in this paper, we are interested in transferring the pose of source human mesh to deform the target human mesh, while the source and target meshes may have different identity information. Traditional studies assume that the paired source and target meshes are existed with the point-wise correspondences of user annotated landmarks/mesh points, which requires heavy labelling efforts. On the other hand, the generalization ability of deep models is limited, when the source and target meshes have different identities. To break this limitation, we proposes the first neural pose transfer model that solves the pose transfer via the latest technique for image style transfer, leveraging the newly proposed component -- spatially adaptive instance normalization. Our model does not require any correspondences between the source and target meshes. Extensive experiments show that the proposed model can effectively transfer deformation from source to target meshes, and has good generalization ability to deal with unseen identities or poses of meshes. Code is available at <a href="https://github.com/jiashunwang/Neural-Pose-Transfer">https://github.com/jiashunwang/Neural-Pose-Transfer</a> .</p>
<hr>
<h2 id="unipose-unified-human-pose-estimation-in-single-images-and-videos">UniPose: Unified Human Pose Estimation in Single Images and Videos</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-89">abstract</h3>
<p>We propose UniPose, a unified framework for human pose estimation, based on our &quot;Waterfall&quot; Atrous Spatial Pooling architecture, that achieves state-of-art-results on several pose estimation metrics. Current pose estimation methods utilizing standard CNN architectures heavily rely on statistical postprocessing or predefined anchor poses for joint localization. UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage, with high accuracy, without relying on statistical postprocessing methods. The Waterfall module in UniPose leverages the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Additionally, our method is extended to UniPose-LSTM for multi-frame processing and achieves state-of-the-art results for temporal pose estimation in Video. Our results on multiple datasets demonstrate that UniPose, with a ResNet backbone and Waterfall module, is a robust and efficient architecture for pose estimation obtaining state-of-the-art results in single person pose detection for both single images and videos.</p>
<hr>
<h2 id="metafuse-a-pre-trained-fusion-model-for-human-pose-estimation">MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-90">abstract</h3>
<p>Cross view feature fusion is the key to address the occlusion problem in human pose estimation. The current fusion methods need to train a separate model for every pair of cameras making them difficult to scale. In this work, we introduce MetaFuse, a pre-trained fusion model learned from a large number of cameras in the Panoptic dataset. The model can be efficiently adapted or finetuned for a new pair of cameras using a small number of labeled images. The strong adaptation power of MetaFuse is due in large part to the proposed factorization of the original fusion model into two parts (1) a generic fusion model shared by all cameras, and (2) lightweight camera-dependent transformations. Furthermore, the generic model is learned from many cameras by a meta-learning style algorithm to maximize its adaptation capability to various camera poses. We observe in experiments that MetaFuse finetuned on the public datasets outperforms the state-of-the-arts by a large margin which validates its value in practice.</p>
<hr>
<h2 id="d3vo-deep-depth-deep-pose-and-deep-uncertainty-for-monocular-visual-odometry">D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-91">abstract</h3>
<p>We propose D3VO as a novel framework for monocular visual odometry that exploits deep networks on three levels -- deep depth, pose and uncertainty estimation. We first propose a novel self-supervised monocular depth estimation network trained on stereo videos without any external supervision. In particular, it aligns the training image pairs into similar lighting condition with predictive brightness transformation parameters. Besides, we model the photometric uncertainties of pixels on the input images, which improves the depth estimation accuracy and provides a learned weighting function for the photometric residuals in direct (feature-less) visual odometry. Evaluation results show that the proposed network outperforms state-of-the-art self-supervised depth estimation networks. D3VO tightly incorporates the predicted depth, pose and uncertainty into a direct visual odometry method to boost both the front-end tracking as well as the back-end non-linear optimization. We evaluate D3VO in terms of monocular visual odometry on both the KITTI odometry benchmark and the EuRoC MAV dataset.The results show that D3VO outperforms state-of-the-art traditional monocular VO methods by a large margin. It also achieves comparable results to state-of-the-art stereo/LiDAR odometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC MAV, while using only a single camera.</p>
<hr>
<h2 id="cross-view-tracking-for-multi-human-3d-pose-estimation-at-over-100-fps">Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-92">abstract</h3>
<p>Estimating 3D poses of multiple humans in real-time is a classic but still challenging task in computer vision. Its major difficulty lies in the ambiguity in cross-view association of 2D poses and the huge state space when there are multiple people in multiple views. In this paper, we present a novel solution for multi-human 3D pose estimation from multiple calibrated camera views. It takes 2D poses in different camera coordinates as inputs and aims for the accurate 3D poses in the global coordinate. Unlike previous methods that associate 2D poses among all pairs of views from scratch at every frame, we exploit the temporal consistency in videos to match the 2D inputs with 3D poses directly in 3-space. More specifically, we propose to retain the 3D pose for each person and update them iteratively via the cross-view multi-human tracking. This novel formulation improves both accuracy and efficiency, as we demonstrated on widely-used public datasets. To further verify the scalability of our method, we propose a new large-scale multi-human dataset with 12 to 28 camera views. Without bells and whistles, our solution achieves 154 FPS on 12 cameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale real-world applications. The proposed dataset will be released soon.</p>
<hr>
<h2 id="where-am-i-looking-at-joint-location-and-orientation-estimation-by-cross-view-matching">Where am I looking at? Joint Location and Orientation Estimation by Cross-View Matching</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-93">abstract</h3>
<p>Cross-view geo-localization is the problem of estimating the position and orientation (latitude, longitude and azimuth angle) of a camera at ground level given a large-scale database of geo-tagged aerial (e.g., satellite) images. Existing approaches treat the task as a pure location estimation problem by learning discriminative feature descriptors, but neglect orientation alignment. It is well-recognized that knowing the orientation between ground and aerial images can significantly reduce matching ambiguity between these two views, especially when the ground-level images have a limited Field of View (FoV) instead of a full field-of-view panorama. Therefore, we design a Dynamic Similarity Matching network to estimate cross-view orientation alignment during localization. In particular, we address the cross-view domain gap by applying a polar transform to the aerial images to approximately align the images up to an unknown azimuth angle. Then, a two-stream convolutional network is used to learn deep features from the ground and polar-transformed aerial images. Finally, we obtain the orientation by computing the correlation between cross-view features, which also provides a more accurate measure of feature similarity, improving location recall. Experiments on standard datasets demonstrate that our method significantly improves state-of-the-art performance. Remarkably, we improve the top-1 location recall rate on the CVUSA dataset by a factor of 1.5x for panoramas with known orientation, by a factor of 3.3x for panoramas with unknown orientation, and by a factor of 6x for 180-degree FoV images with unknown orientation.</p>
<hr>
<h2 id="bodies-at-rest-3d-human-pose-and-shape-estimation-from-a-pressure-image-using-synthetic-data">Bodies at Rest: 3D Human Pose and Shape Estimation from a Pressure Image using Synthetic Data</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-94">abstract</h3>
<p>People spend a substantial part of their lives at rest in bed. 3D human pose and shape estimation for this activity would have numerous beneficial applications, yet line-of-sight perception is complicated by occlusion from bedding. Pressure sensing mats are a promising alternative, but training data is challenging to collect at scale. We describe a physics-based method that simulates human bodies at rest in a bed with a pressure sensing mat, and present PressurePose, a synthetic dataset with 206K pressure images with 3D human poses and shapes. We also present PressureNet, a deep learning model that estimates human pose and shape given a pressure image and gender. PressureNet incorporates a pressure map reconstruction (PMR) network that models pressure image generation to promote consistency between estimated 3D body models and pressure image input. In our evaluations, PressureNet performed well with real data from participants in diverse poses, even though it had only been trained with synthetic data. When we ablated the PMR network, performance dropped substantially.</p>
<hr>
<h2 id="accurate-estimation-of-body-height-from-a-single-depth-image-via-a-four-stage-developing-network">Accurate Estimation of Body Height from a Single Depth Image via a Four-Stage Developing Network</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-95">abstract</h3>
<p>Non-contact measurement ofhuman body height can be
very difficult under some circumstances. In this paper we address the problem of accurately estimating the height of a person with arbitrary postures from a single depth image. By introducing a novel part-based intermediate represen- tation plus a four-stage increasingly complex deep neural network, we manage to achieve significantly higher accura- cy than previous methods. We first describe the human body in the form ofa segmentation ofhuman torso as four nearly rigid parts and then predict their lengths respectively by 3 CNNs. Instead ofdirectly adding the lengths ofthese parts together, we further construct another independent devel- oping CNN that combines the intermediate representation, part lengths and depth information together to finally pre- dict the body height results.Here we develop an increasingly complex network architecture and adopt a hybrid pooling to optimize training process. To the best ofour knowledge, this is the first method that estimates height only from a single depth image. In experiments our average accuracy reaches at 99.1% for people in various positions and postures.</p>
<hr>
<h2 id="deepfaceflow-in-the-wild-dense-3d-facial-motion-estimation">DeepFaceFlow: In-the-wild Dense 3D Facial Motion Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-96">abstract</h3>
<p>Dense 3D facial motion capture from only monocular in-the-wild pairs of RGB images is a highly challenging problem with numerous applications, ranging from facial expression recognition to facial reenactment. In this work, we propose DeepFaceFlow, a robust, fast, and highly-accurate framework for the dense estimation of 3D non-rigid facial flow between pairs of monocular images. Our DeepFaceFlow framework was trained and tested on two very large-scale facial video datasets, one of them of our own collection and annotation, with the aid of occlusion-aware and 3D-based loss function. We conduct comprehensive experiments probing different aspects of our approach and demonstrating its improved performance against state-of-the-art flow and 3D reconstruction methods. Furthermore, we incorporate our framework in a full-head state-of-the-art facial video synthesis method and demonstrate the ability of our method in better representing and capturing the facial dynamics, resulting in a highly-realistic facial video synthesis. Given registered pairs of images, our framework generates 3D flow maps at ~60 fps.</p>
<hr>
<h2 id="towards-better-generalization-joint-depth-pose-learning-without-posenet">Towards Better Generalization: Joint Depth-Pose Learning without PoseNet</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-97">abstract</h3>
<p>In this work, we tackle the essential problem of scale inconsistency for self-supervised joint depth-pose learning. Most existing methods assume that a consistent scale of depth and pose can be learned across all input samples, which makes the learning problem harder, resulting in degraded performance and limited generalization in indoor environments and long-sequence visual odometry application. To address this issue, we propose a novel system that explicitly disentangles scale from the network estimation. Instead of relying on PoseNet architecture, our method recovers relative pose by directly solving fundamental matrix from dense optical flow correspondence and makes use of a two-view triangulation module to recover an up-to-scale 3D structure. Then, we align the scale of the depth prediction with the triangulated point cloud and use the transformed depth map for depth error computation and dense reprojection check. Our whole system can be jointly trained end-to-end. Extensive experiments show that our system not only reaches state-of-the-art performance on KITTI depth and flow estimation, but also significantly improves the generalization ability of existing self-supervised depth-pose learning methods under a variety of challenging scenarios, and achieves state-of-the-art results among self-supervised learning-based methods on KITTI Odometry and NYUv2 dataset. Furthermore, we present some interesting findings on the limitation of PoseNet-based relative pose estimation methods in terms of generalization ability. Code is available at <a href="https://github.com/B1ueber2y/TrianFlow">https://github.com/B1ueber2y/TrianFlow</a>.</p>
<hr>
<h2 id="weakly-supervised-3d-human-pose-learning-via-multi-view-images-in-the-wild">Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-98">abstract</h3>
<p>One major challenge for monocular 3D human pose estimation in-the-wild is the acquisition of training data that contains unconstrained images annotated with accurate 3D poses. In this paper, we address this challenge by proposing a weakly-supervised approach that does not require 3D annotations and learns to estimate 3D poses from unlabeled multi-view data, which can be acquired easily in in-the-wild environments. We propose a novel end-to-end learning framework that enables weakly-supervised training using multi-view consistency. Since multi-view consistency is prone to degenerated solutions, we adopt a 2.5D pose representation and propose a novel objective function that can only be minimized when the predictions of the trained model are consistent and plausible across all camera views. We evaluate our proposed approach on two large scale datasets (Human3.6M and MPII-INF-3DHP) where it achieves state-of-the-art performance among semi-/weakly-supervised methods.</p>
<hr>
<h2 id="deep-kinematics-analysis-for-monocular-3d-human-pose-estimation">Deep Kinematics Analysis for Monocular 3D Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-99">abstract</h3>
<p>For monocular 3D pose estimation conditioned on 2D detection, noisy/unreliable input is a key obstacle in this task. Simple structure constraints attempting to tackle this problem, e.g., symmetry loss and joint angle limit, could only provide marginal improvements and are commonly treated as auxiliary losses in previous researches. It still remains challenging to fully utilize human prior knowledge in this task. In this paper, we propose to address above issue in a systematic view. Firstly, we show that optimizing the kinematics structure of noisy 2D inputs is critical to obtain accurate 3D estimations. Secondly, based on corrected 2D joints, we further explicitly decompose articulated motion with human topology, which leads to more compact 3D static structure easier for estimation. Finally, we propose a temporal module to refine 3D trajectories, which obtains more rational results. Above three steps are seamlessly integrated into deep neural models, which form a deep kinematics analysis pipeline concurrently considering the static/dynamic structure of 2D inputs and 3D outputs. Extensive experiments show that proposed framework achieves state-of-the-art performance on two widely used 3D human action datasets. Meanwhile, targeted ablation study shows that each former step is critical for the latter one to obtain promising results.</p>
<hr>
<h2 id="compressed-volumetric-heatmaps-for-multi-person-3d-pose-estimation">Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-100">abstract</h3>
<p>In this paper we present a novel approach for bottom-up multi-person 3D human pose estimation from monocular RGB images. We propose to use high resolution volumetric heatmaps to model joint locations, devising a simple and effective compression method to drastically reduce the size of this representation. At the core of the proposed method lies our Volumetric Heatmap Autoencoder, a fully-convolutional network tasked with the compression of ground-truth heatmaps into a dense intermediate representation. A second model, the Code Predictor, is then trained to predict these codes, which can be decompressed at test time to re-obtain the original representation. Our experimental evaluation shows that our method performs favorably when compared to state of the art on both multi-person and single-person 3D human pose estimation datasets and, thanks to our novel compression strategy, can process full-HD images at the constant runtime of 8 fps regardless of the number of subjects in the scene. Code and models available at <a href="https://github.com/fabbrimatteo/LoCO">https://github.com/fabbrimatteo/LoCO</a> .</p>
<hr>
<h2 id="correlating-edge-pose-with-parsing">Correlating Edge, Pose with Parsing</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-101">abstract</h3>
<p>According to existing studies, human body edge and pose are two beneficial factors to human parsing. The effectiveness of each of the high-level features (edge and pose) is confirmed through the concatenation of their features with the parsing features. Driven by the insights, this paper studies how human semantic boundaries and keypoint locations can jointly improve human parsing. Compared with the existing practice of feature concatenation, we find that uncovering the correlation among the three factors is a superior way of leveraging the pivotal contextual cues provided by edges and poses. To capture such correlations, we propose a Correlation Parsing Machine (CorrPM) employing a heterogeneous non-local block to discover the spatial affinity among feature maps from the edge, pose and parsing. The proposed CorrPM allows us to report new state-of-the-art accuracy on three human parsing datasets. Importantly, comparative studies confirm the advantages of feature correlation over the concatenation.</p>
<hr>
<h2 id="ghum--ghuml-generative-3d-human-shape-and-articulated-pose-models">GHUM &amp; GHUML: Generative 3D Human Shape and Articulated Pose Models</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-102">abstract</h3>
<p>We present a statistical, articulated 3D human shape modeling pipeline, within a fully trainable, modular, deep learning framework. Given high-resolution complete 3D body scans of humans, captured in various poses, together with additional closeups of their head and facial expressions , as well as hand articulation, and given initial, artist designed, gender neutral rigged quad-meshes, we train all model parameters including non-linear shape spaces based on variational auto-encoders, pose-space deformation cor-rectives, skeleton joint center predictors, and blend skinning functions, in a single consistent learning loop. The models are simultaneously trained with all the 3d dynamic scan data (over 60, 000 diverse human configurations in our new dataset) in order to capture correlations and ensure consistency of various components. Models support facial expression analysis, as well as body (with detailed hand) shape and pose estimation. We provide fully train-able generic human models of different resolutions-the moderate-resolution GHUM consisting of 10,168 vertices and the low-resolution GHUML(ite) of 3,194 vertices-, run comparisons between them, analyze the impact of different components and illustrate their reconstruction from image data. The models will be available for research.</p>
<hr>
<h2 id="cascaded-deep-monocular-3d-human-pose-estimation-with-evolutionary-training-data">Cascaded deep monocular 3D human pose estimation with evolutionary training data</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-103">abstract</h3>
<p>End-to-end deep representation learning has achieved remarkable accuracy for monocular 3D human pose estimation, yet these models may fail for unseen poses with limited and fixed training data. This paper proposes a novel data augmentation method that: (1) is scalable for synthesizing massive amount of training data (over 8 million valid 3D human poses with corresponding 2D projections) for training 2D-to-3D networks, (2) can effectively reduce dataset bias. Our method evolves a limited dataset to synthesize unseen 3D human skeletons based on a hierarchical human representation and heuristics inspired by prior knowledge. Extensive experiments show that our approach not only achieves state-of-the-art accuracy on the largest public benchmark, but also generalizes significantly better to unseen and rare poses. Relevant files and tools are available at the project website.</p>
<hr>
<h2 id="self-supervised-3d-human-pose-estimation-via-part-guided-novel-image-synthesis">Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-104">abstract</h3>
<p>Camera captured human pose is an outcome of several sources of variation. Performance of supervised 3D pose estimation approaches comes at the cost of dispensing with variations, such as shape and appearance, that may be useful for solving other related tasks. As a result, the learned model not only inculcates task-bias but also dataset-bias because of its strong reliance on the annotated samples, which also holds true for weakly-supervised models. Acknowledging this, we propose a self-supervised learning framework to disentangle such variations from unlabeled video frames. We leverage the prior knowledge on human skeleton and poses in the form of a single part-based 2D puppet model, human pose articulation constraints, and a set of unpaired 3D poses. Our differentiable formalization, bridging the representation gap between the 3D pose and spatial part maps, not only facilitates discovery of interpretable pose disentanglement but also allows us to operate on videos with diverse camera movements. Qualitative results on unseen in-the-wild datasets establish our superior generalization across multiple tasks beyond the primary tasks of 3D pose estimation and part segmentation. Furthermore, we demonstrate state-of-the-art weakly-supervised 3D pose estimation performance on both Human3.6M and MPI-INF-3DHP datasets.</p>
<hr>
<h2 id="combining-detection-and-tracking-for-human-pose-estimation-in-videos">Combining detection and tracking for human pose estimation in videos</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-105">abstract</h3>
<p>We propose a novel top-down approach that tackles the problem of multi-person human pose estimation and tracking in videos. In contrast to existing top-down approaches, our method is not limited by the performance of its person detector and can predict the poses of person instances not localized. It achieves this capability by propagating known person locations forward and backward in time and searching for poses in those regions. Our approach consists of three components: (i) a Clip Tracking Network that performs body joint detection and tracking simultaneously on small video clips; (ii) a Video Tracking Pipeline that merges the fixed-length tracklets produced by the Clip Tracking Network to arbitrary length tracks; and (iii) a Spatial-Temporal Merging procedure that refines the joint locations based on spatial and temporal smoothing terms. Thanks to the precision of our Clip Tracking Network and our merging procedure, our approach produces very accurate joint predictions and can fix common mistakes on hard scenarios like heavily entangled people. Our approach achieves state-of-the-art results on both joint detection and tracking, on both the PoseTrack 2017 and 2018 datasets, and against all top-down and bottom-down approaches.</p>
<hr>
<h2 id="hemlets-pose-learning-part-centric-heatmap-triplets-for-accurate-3d-human-pose-estimation">HEMlets pose: Learning part-centric heatmap triplets for accurate 3D human pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-106">abstract</h3>
<p>Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state - Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network(ConvNet) is first trained to predict HEMlests from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade method (by 20% on Human3.6M). The proposed method naturally supports training with 'in-the-wild'' images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images.</p>
<hr>
<h2 id="polarimetric-relative-pose-estimation-zhaopeng">Polarimetric Relative Pose Estimation Zhaopeng</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-107">abstract</h3>
<p>In this paper we consider the problem of relative pose estimation from two images with per-pixel polarimetric information. Using these additional measurements we derive a simple minimal solver for the essential matrix which only requires two point correspondences. The polarization constraints allow us to pointwise recover the 3D surface normal up to a two-fold ambiguity for the diffuse reflection. Since this ambiguity exists per point, there is a combinatorial explosion of possibilities. However, since our solver only requires two point correspondences, we only need to consider 16 configurations when solving for the relative pose. Once the relative orientation is recovered, we show that it is trivial to resolve the ambiguity for the remaining points. For robustness, we also propose a joint optimization between the relative pose and the refractive index to handle the refractive distortion. In experiments, on both synthetic and real data, we demonstrate that by leveraging the additional information available from polarization cameras, we can improve over classical methods which only rely on the 2D-point locations to estimate the geometry. Finally, we demonstrate the practical applicability of our approach by integrating it into a state-of-the-art global Structure-from-Motion pipeline.</p>
<hr>
<h2 id="camera-distance-aware-top-down-approach-for-3d-multi-person-pose-estimation-from-a-single-rgb-image">Camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-108">abstract</h3>
<p>Although significant improvement has been achieved recently in 3D human pose estimation, most of the previous methods only treat a single-person case. In this work, we firstly propose a fully learning-based, camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image. The pipeline of the proposed system consists of human detection, absolute 3D human root localization, and root-relative 3D single-person pose estimation modules. Our system achieves comparable results with the state-of-the-art 3D single-person pose estimation models without any groundtruth information and significantly outperforms previous 3D multi-person pose estimation methods on publicly available datasets. The code is available in footnote{url{<a href="https://github.com/mks0601/3DMPPE-ROOTNET-RELEASE">https://github.com/mks0601/3DMPPE-ROOTNET-RELEASE</a>}}textsuperscript{,}footnote{url{<a href="https://github.com/mks0601/3DMPPE-POSENET-RELEASE">https://github.com/mks0601/3DMPPE-POSENET-RELEASE</a>}}.</p>
<hr>
<h2 id="resolving-3d-human-pose-ambiguities-with-3d-scene-constraints">Resolving 3D human pose ambiguities with 3D scene constraints</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-109">abstract</h3>
<p>To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The inter-penetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at <a href="https://prox.is.tue.mpg.de">https://prox.is.tue.mpg.de</a>.</p>
<hr>
<h2 id="on-boosting-single-frame-3d-human-pose-estimation-via-monocular-videos">On boosting single-frame 3D human pose estimation via monocular videos</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-110">abstract</h3>
<p>The premise of training an accurate 3D human pose estimation network is the possession of huge amount of richly annotated training data. Nonetheless, manually obtaining rich and accurate annotations is, even not impossible, tedious and slow. In this paper, we propose to exploit monocular videos to complement the training dataset for the single-image 3D human pose estimation tasks. At the beginning, a baseline model is trained with a small set of annotations. By fixing some reliable estimations produced by the resulting model, our method automatically collects the annotations across the entire video as solving the 3D trajectory completion problem. Then, the baseline model is further trained with the collected annotations to learn the new poses. We evaluate our method on the broadly-adopted Human3.6M and MPI-INF-3DHP datasets. As illustrated in experiments, given only a small set of annotations, our method successfully makes the model to learn new poses from unlabelled monocular videos, promoting the accuracies of the baseline model by about 10%. By contrast with previous approaches, our method does not rely on either multi-view imagery or any explicit 2D keypoint annotations.</p>
<hr>
<h2 id="chirality-nets-for-human-pose-regression">Chirality Nets for Human Pose Regression</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-111">abstract</h3>
<p>We propose Chirality Nets, a family of deep nets that is equivariant to the &quot;chirality transform,&quot; i.e., the transformation to create a chiral pair. Through parameter sharing, odd and even symmetry, we propose and prove variants of standard building blocks of deep nets that satisfy the equivariance property, including fully connected layers, convolutional layers, batch-normalization, and LSTM/GRU cells. The proposed layers lead to a more data efficient representation and a reduction in computation by exploiting symmetry. We evaluate chirality nets on the task of human pose regression, which naturally exploits the left/right mirroring of the human body. We study three pose regression tasks: 3D pose estimation from video, 2D pose forecasting, and skeleton based activity recognition. Our approach achieves/matches state-of-the-art results, with more significant gains on small datasets and limited-data settings.</p>
<hr>
<h2 id="sim2real-transfer-learning-for-3d-human-pose-estimation-motion-to-the-rescue">Sim2real transfer learning for 3D human pose estimation: motion to the rescue</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-112">abstract</h3>
<p>Synthetic visual data can provide practically infinite diversity and rich labels, while avoiding ethical issues with privacy and bias. However, for many tasks, current models trained on synthetic data generalize poorly to real data. The task of 3D human pose estimation is a particularly interesting example of this sim2real problem, because learning-based approaches perform reasonably well given real training data, yet labeled 3D poses are extremely difficult to obtain in the wild, limiting scalability. In this paper, we show that standard neural-network approaches, which perform poorly when trained on synthetic RGB images, can perform well when the data is pre-processed to extract cues about the person's motion, notably as optical flow and the motion of 2D keypoints. Therefore, our results suggest that motion can be a simple way to bridge a sim2real gap when video is available. We evaluate on the 3D Poses in the Wild dataset, the most challenging modern benchmark for 3D pose estimation, where we show full 3D mesh recovery that is on par with state-of-the-art methods trained on real 3D sequences, despite training only on synthetic humans from the SURREAL dataset.</p>
<hr>
<h2 id="cross-view-person-identification-by-matching-human-poses-estimated-with-confidence-on-each-body-joint">Cross-view person identification by matching human poses estimated with confidence on each body joint</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-113">abstract</h3>
<p>Cross-view person identification (CVPI) from multiple temporally synchronized videos taken by multiple wearable cameras from different, varying views is a very challenging but important problem, which has attracted more interests recently. Current state-of-the-art performance of CVPI is achieved by matching appearance and motion features across videos, while the matching of pose features does not work effectively given the high inaccuracy of the 3D human pose estimation on videos/images collected in the wild. In this paper, we introduce a new metric of confidence to the 3D human pose estimation and show that the combination of the inaccurately estimated human pose and the inferred confidence metric can be used to boost the CVPI performance -the estimated pose information can be integrated to the appearance and motion features to achieve the new state-of-the-art CVPI performance. More specifically, the estimated confidence metric is measured at each human-body joint and the joints with higher confidence are weighted more in the pose matching for CVPI. In the experiments, we validate the proposed method on three wearable-camera video datasets and compare the performance against several other existing CVPI methods.</p>
<hr>
<h2 id="a-cascaded-inception-of-inception-network-with-attention-modulated-feature-fusion-for-human-pose-estimation">A cascaded inception of inception network with attention modulated feature fusion for human pose estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-114">abstract</h3>
<p>Accurate keypoint localization of human pose needs diversified features: the high level for contextual dependencies and the low level for detailed refinement of joints. However, the importance of the two factors varies from case to case, but how to efficiently use the features is still an open problem. Existing methods have limitations in preserving low level features, adaptively adjusting the importance of different levels of features, and modeling the human perception process. This paper presents three novel techniques step by step to efficiently utilize different levels of features for human pose estimation. Firstly, an inception of inception (IOI) block is designed to emphasize the low level features. Secondly, an attention mechanism is proposed to adjust the importance of individual levels according to the context. Thirdly, a cascaded network is proposed to sequentially localize the joints to enforce message passing from joints of stand-alone parts like head and torso to remote joints like wrist or ankle. Experimental results demonstrate that the proposed method achieves the state-of-the-art performance on both MPII and LSP benchmarks.</p>
<hr>
<h2 id="ego-pose-estimation-and-forecasting-as-real-time-pd-control">Ego-pose estimation and forecasting as real-time PD control</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-115">abstract</h3>
<p>We propose the use of a proportional-derivative (PD) control based policy learned via reinforcement learning (RL) to estimate and forecast 3D human pose from egocentric videos. The method learns directly from unsegmented egocentric videos and motion capture data consisting of various complex human motions (e.g., crouching, hopping, bending, and motion transitions). We propose a video-conditioned recurrent control technique to forecast physically-valid and stable future motions of arbitrary length. We also introduce a value function based fail-safe mechanism which enables our method to run as a single pass algorithm over the video data. Experiments with both controlled and in-the-wild data show that our approach outperforms previous art in both quantitative metrics and visual quality of the motions, and is also robust enough to transfer directly to real-world scenarios. Additionally, our time analysis shows that the combined use of our pose estimation and forecasting can run at 30 FPS, making it suitable for real-time applications.</p>
<hr>
<h2 id="end-to-end-learning-for-graph-decomposition-1">End-to-end learning for graph decomposition</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-116">abstract</h3>
<p>Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation.</p>
<hr>
<h2 id="simple-baselines-for-human-pose-estimation-and-tracking-1">Simple baselines for human pose estimation and tracking</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-117">abstract</h3>
<p>There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at <a href="https://github.com/leoxiaobin/pose.pytorch">https://github.com/leoxiaobin/pose.pytorch</a>.</p>
<hr>
<h2 id="monocular-3d-pose-and-shape-estimation-of-multiple-people-in-natural-scenes-the-importance-of-multiple-scene-constraints-1">Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes: The Importance of Multiple Scene Constraints</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-118">abstract</h3>
<p>Human sensing has greatly benefited from recent advances in deep learning, parametric human modeling, and large scale 2d and 3d datasets. However, existing 3d models make strong assumptions about the scene, considering either a single person per image, full views of the person, a simple background or many cameras. In this paper, we leverage state-of-the-art deep multi-task neural networks and parametric human and scene modeling, towards a fully automatic monocular visual sensing system for multiple interacting people, which (i) infers the 2d and 3d pose and shape of multiple people from a single image, relying on detailed semantic representations at both model and image level, to guide a combined optimization with feedforward and feedback components, (ii) automatically integrates scene constraints including ground plane support and simultaneous volume occupancy by multiple people, and (iii) extends the single image model to video by optimally solving the temporal person assignment problem and imposing coherent temporal pose and motion reconstructions while preserving image alignment fidelity. We perform experiments on both single and multi-person datasets, and systematically evaluate each component of the model, showing improved performance and extensive multiple human sensing capability. We also apply our method to images with multiple people, severe occlusions and diverse backgrounds captured in challenging natural scenes, and obtain results of good perceptual quality.</p>
<hr>
<h2 id="efficient-online-multi-person-2d-pose-tracking-with-recurrent-spatio-temporal-affinity-fields">Efficient online multi-person 2D pose tracking with recurrent spatio-temporal affinity fields</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-119">abstract</h3>
<p>We present an online approach to efficiently and simultaneously detect and track 2D poses of multiple people in a video sequence. We build upon Part Affinity Field (PAF) representation designed for static images, and propose an architecture that can encode and predict Spatio-Temporal Affinity Fields (STAF) across a video sequence. In particular, we propose a novel temporal topology cross-linked across limbs which can consistently handle body motions of a wide range of magnitudes. Additionally, we make the overall approach recurrent in nature, where the network ingests STAF heatmaps from previous frames and estimates those for the current frame. Our approach uses only online inference and tracking, and is currently the fastest and the most accurate bottom-up approach that is runtime-invariant to the number of people in the scene and accuracy-invariant to input frame rate of camera. Running at sim30 fps on a single GPU at single scale, it achieves highly competitive results on the PoseTrack benchmarks.</p>
<hr>
<h2 id="multi-scale-structure-aware-network-for-human-pose-estimation">Multi-Scale Structure-Aware Network for Human Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-120">abstract</h3>
<p>We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.</p>
<hr>
<h2 id="cascaded-pyramid-network-for-multi-person-pose-estimation-1">Cascaded Pyramid Network for Multi-person Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-121">abstract</h3>
<p>The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these 'hard' keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the 'simple' keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the 'hard' keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code1 and the detection results for person used will be publicly available for further research.</p>
<hr>
<h2 id="training-cnns-with-normalized-kernels">Training CNNs with normalized kernels</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-122">abstract</h3>
<p>Several methods of normalizing convolution kernels have been proposed in the literature to train convolutional neural networks (CNNs), and have shown some success. However, our understanding of these methods has lagged behind their success in application; there are a lot of open questions, such as why a certain type of kernel normalization is effective and what type of normalization should be employed for each (e.g., higher or lower) layer of a CNN. As the first step towards answering these questions, we propose a framework that enables us to use a variety of kernel normalization methods at any layer of a CNN. A naive integration of kernel normalization with a general optimization method, such as SGD, often entails instability while updating parameters. Thus, existing methods employ ad-hoc procedures to empirically assure convergence. In this study, we pose estimation of convolution kernels under normalization constraints as constraint-free optimization on kernel submanifolds that are identified by the employed constraints. Note that naive application of the established optimization methods for matrix manifolds to the aforementioned problems is not feasible because of the hierarchical nature of CNNs. To this end, we propose an algorithm for optimization on kernel manifolds in CNNs by appropriate scaling of the space of kernels based on structure of CNNs and statistics of data. We theoretically prove that the proposed algorithm has assurance of almost sure convergence to a solution at single minimum. Our experimental results show that the proposed method can successfully train popular CNN models using several different types of kernel normalization methods. Moreover, they show that the proposed method improves classification performance of baseline CNNs, and provides state-of-the-art performance for major image classification benchmarks.</p>
<hr>
<h2 id="towards-multi-pose-guided-virtual-try-on-network">Towards multi-pose guided virtual try-on network</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-123">abstract</h3>
<p>Virtual try-on systems under arbitrary human poses have significant application potential, yet also raise extensive challenges, such as self-occlusions, heavy misalignment among different poses, and complex clothes textures. Existing virtual try-on methods can only transfer clothes given a fixed human pose, and still show unsatisfactory performances, often failing to preserve person identity or texture details, and with limited pose diversity. This paper makes the first attempt towards a multi-pose guided virtual try-on system, which enables clothes to transfer onto a person with diverse poses. Given an input person image, a desired clothes image, and a desired pose, the proposed Multi-pose Guided Virtual Try-On Network (MG-VTON) generates a new person image after fitting the desired clothes into the person and manipulating the pose. MG-VTON is constructed with three stages: 1) a conditional human parsing network is proposed that matches both the desired pose and the desired clothes shape; 2) a deep Warping Generative Adversarial Network (Warp-GAN) that warps the desired clothes appearance into the synthesized human parsing map and alleviates the misalignment problem between the input human pose and the desired one; 3) a refinement render network recovers the texture details of clothes and removes artifacts, based on multi-pose composition masks. Extensive experiments on commonly-used datasets and our newly-collected largest virtual try-on benchmark demonstrate that our MG-VTON significantly outperforms all state-of-the-art methods both qualitatively and quantitatively, showing promising virtual try-on performances.</p>
<hr>
<h2 id="posetrack-a-benchmark-for-human-pose-estimation-and-tracking">PoseTrack: A Benchmark for Human Pose Estimation and Tracking</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-124">abstract</h3>
<p>Existing systems for video-based pose estimation and tracking struggle to perform well on realistic videos with multiple people and often fail to output body-pose trajectories consistent over time. To address this shortcoming this paper introduces PoseTrack which is a new large-scale benchmark for video-based human pose estimation and articulated tracking. Our new benchmark encompasses three tasks focusing on i) single-frame multi-person pose estimation, ii) multi-person pose estimation in videos, and iii) multi-person articulated tracking. To establish the benchmark, we collect, annotate and release a new dataset that features videos with multiple people labeled with person tracks and articulated pose. A public centralized evaluation server is provided to allow the research community to evaluate on a held-out test set. Furthermore, we conduct an extensive experimental study on recent approaches to articulated pose tracking and provide analysis of the strengths and weaknesses of the state of the art. We envision that the proposed benchmark will stimulate productive research both by providing a large and representative training dataset as well as providing a platform to objectively evaluate and compare the proposed methods. The benchmark is freely accessible at <a href="https://posetrack.net/">https://posetrack.net/</a>.</p>
<hr>
<h2 id="pandanet--anchor-based-single-shot-multi-person-3d-pose-estimation">PandaNet : Anchor-Based Single-Shot Multi-Person 3D Pose Estimation</h2>
<p><a href="https">pdf</a></p>
<h3 id="abstract-125">abstract</h3>
<p>Recently, several deep learning models have been pro-
posed for 3D human pose estimation. Nevertheless, most of these approaches only focus on the single-person case or estimate 3D pose of a few people at high resolution. Furthermore, many applications such as autonomous driv- ing or crowd analysis require pose estimation of a large number of people possibly at low-resolution. In this work, we present PandaNet (Pose estimAtioN and Dectection Anchor-based Network), a new single-shot, anchor-based and multi-person 3D pose estimation approach. The pro- posed model performs bounding box detection and, for each detected person, 2D and 3D pose regression into a single forward pass. It does not need any post-processing to re- group joints since the network predicts a full 3D pose for each bounding box and allows the pose estimation ofa pos- sibly large number of people at low resolution. To manage people overlapping, we introduce a Pose-Aware Anchor Se- lection strategy. Moreover, as imbalance exists between dif- ferent people sizes in the image, and joints coordinates have
different uncertainties depending on these sizes, we pro- pose a method to automatically optimize weights associated to different people scales and joints for efficient training. PandaNet surpasses previous single-shot methods on sev- eral challenging datasets: a multi-person urban virtual but very realistic dataset (JTA Dataset), and two real world 3D multi-person datasets (CMU Panoptic and MuPoTS-3D).
1.</p>
<hr>

    </body>
    </html>